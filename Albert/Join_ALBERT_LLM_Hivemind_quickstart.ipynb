{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDRyPYFJlIy0",
        "outputId": "ab5948fa-10f6-4b97-cae9-ecd3c6f04421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 16:19:35--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 13.248.244.96, 99.83.220.108, 75.2.60.68, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|13.248.244.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14796857 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]  14.11M  6.65MB/s    in 2.1s    \n",
            "\n",
            "2024-12-11 16:19:38 (6.65 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz’ saved [14796857/14796857]\n",
            "\n",
            "ngrok\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "# ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "\n",
        "# 替换 YOUR_NGROK_AUTH_TOKEN 为你的 ngrok 认证令牌\n",
        "!./ngrok authtoken 2q0WqdIL8HEbZMSU8CXwTFZrR5y_2A9nKPfF8zj8mC2tGkiF1\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DtA5PA1yakb",
        "outputId": "32a28685-65dc-4e2d-86ec-8e72cee45726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
            "Collecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (2.32.3)\n",
            "Collecting torch==1.11.0 (from torchvision==0.12.0)\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2024.8.30)\n",
            "Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0 torchvision-0.12.0\n",
            "1.11.0+cu102\n",
            "0.12.0+cu102\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1\n",
        "!pip install torchvision==0.12.0\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VmBR9wXRhmhT",
        "outputId": "21f5d2e2-830d-42f4-ae00-0775e5e014ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers~=4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.46.3)\n",
            "Collecting datasets~=1.5 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-1.18.4-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting torch_optimizer==0.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading torch_optimizer-0.1.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.10.26 (from -r requirements.txt (line 4))\n",
            "  Downloading wandb-0.10.26-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.32.3)\n",
            "Collecting nltk==3.6.7 (from -r requirements.txt (line 7))\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer==0.1.0->-r requirements.txt (line 3)) (1.11.0)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer==0.1.0->-r requirements.txt (line 3))\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (3.1.43)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb==0.10.26->-r requirements.txt (line 4))\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (2.19.0)\n",
            "Collecting subprocess32>=3.5.3 (from wandb==0.10.26->-r requirements.txt (line 4))\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (0.4.0)\n",
            "Collecting configparser>=3.8.1 (from wandb==0.10.26->-r requirements.txt (line 4))\n",
            "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (5.29.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.10.26->-r requirements.txt (line 4)) (6.0.2)\n",
            "Collecting pathtools (from wandb==0.10.26->-r requirements.txt (line 4))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.7->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.7->-r requirements.txt (line 7)) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.7->-r requirements.txt (line 7)) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.6->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (17.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets~=1.5->-r requirements.txt (line 2)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=1.5->-r requirements.txt (line 2)) (3.11.9)\n",
            "Collecting responses<0.19 (from datasets~=1.5->-r requirements.txt (line 2))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 6)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 6)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 6)) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=1.5->-r requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=1.0.0->wandb==0.10.26->-r requirements.txt (line 4)) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers~=4.6->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets~=1.5->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets~=1.5->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.10.26->-r requirements.txt (line 4)) (5.0.1)\n",
            "Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.10.26-py2.py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6489 sha256=b0288d4306f64165d632227d14aeec8ad232a3ee7fb669eda2ab665184c8aab6\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/19/61/d440ccd46a2a014bce61fc5c6c8495dedd32ef04cba8b34b28\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=b6511b7a62dbbaa8fa59fb05d017c5b27985751383d1723d4b0e9b57f239c66e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: pathtools, subprocess32, shortuuid, nltk, configparser, responses, pytorch-ranger, wandb, torch_optimizer, datasets\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.0\n",
            "    Uninstalling wandb-0.19.0:\n",
            "      Successfully uninstalled wandb-0.19.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.2.0\n",
            "    Uninstalling datasets-3.2.0:\n",
            "      Successfully uninstalled datasets-3.2.0\n",
            "Successfully installed configparser-7.1.0 datasets-1.18.4 nltk-3.6.7 pathtools-0.1.2 pytorch-ranger-0.1.1 responses-0.18.0 shortuuid-1.0.13 subprocess32-3.5.4 torch_optimizer-0.1.0 wandb-0.10.26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "backports"
                ]
              },
              "id": "b7b61e09c51c46288aa34a1d090fd306"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJOXs--OjqDh",
        "outputId": "39f7f9a3-beed-4bad-b09b-aa286248143f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (1.18.4)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 1.18.4\n",
            "    Uninstalling datasets-1.18.4:\n",
            "      Successfully uninstalled datasets-1.18.4\n",
            "Successfully installed datasets-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xwPQUzdNX6CE"
      },
      "outputs": [],
      "source": [
        "!chmod 755 ./tokenize_wikitext103.py\n",
        "!chmod 755 ./run_training_monitor.py\n",
        "!chmod 755 ./run_trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUElr5WkVKRp",
        "outputId": "1ca05e75-c999-48ca-df94-1d0f57e6728a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 173kB/s]\n",
            "spiece.model: 100% 760k/760k [00:00<00:00, 1.60MB/s]\n",
            "tokenizer.json: 100% 1.31M/1.31M [00:00<00:00, 2.00MB/s]\n",
            "config.json: 100% 685/685 [00:00<00:00, 4.77MB/s]\n",
            "README.md: 100% 10.5k/10.5k [00:00<00:00, 41.4MB/s]\n",
            "test-00000-of-00001.parquet: 100% 722k/722k [00:00<00:00, 39.4MB/s]\n",
            "train-00000-of-00002.parquet: 100% 156M/156M [00:00<00:00, 246MB/s]\n",
            "train-00001-of-00002.parquet: 100% 156M/156M [00:00<00:00, 246MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 655k/655k [00:00<00:00, 125MB/s]\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 117294.20 examples/s]\n",
            "Generating train split: 100% 1801350/1801350 [00:01<00:00, 911224.47 examples/s]\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 628836.20 examples/s]\n",
            "Map (num_proc=8): 100% 4358/4358 [00:01<00:00, 3750.23 examples/s]\n",
            "Map (num_proc=8):  37% 669000/1801350 [01:55<03:46, 4995.70 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=8):  86% 1549000/1801350 [04:26<00:33, 7601.31 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=8): 100% 1801350/1801350 [05:12<00:00, 5756.74 examples/s]\n",
            "Map (num_proc=8): 100% 3760/3760 [00:01<00:00, 3641.25 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 1723/1723 [00:00<00:00, 140217.03 examples/s]\n",
            "Saving the dataset (2/2 shards): 100% 711735/711735 [00:01<00:00, 504747.46 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 1557/1557 [00:00<00:00, 182269.43 examples/s]\n"
          ]
        }
      ],
      "source": [
        "!./tokenize_wikitext103.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdeu1yfaAxxD",
        "outputId": "ddfbf429-5c2b-4a28-dba6-933303ba0c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/learning-at-home/hivemind.git\n",
            "  Cloning https://github.com/learning-at-home/hivemind.git to /tmp/pip-req-build-sxu888gf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/learning-at-home/hivemind.git /tmp/pip-req-build-sxu888gf\n",
            "  Resolved https://github.com/learning-at-home/hivemind.git to commit 10f82ee9e080c4fdbc95f2f26971a7afd55f43fa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce (from hivemind==1.2.0.dev0)\n",
            "  Using cached multiaddr-0.0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: prefetch_generator>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.0.3)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: uvloop>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.68.1)\n",
            "Collecting protobuf<5.28.0,>=3.12.2 (from hivemind==1.2.0.dev0)\n",
            "  Using cached protobuf-5.27.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: configargparse>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (1.7)\n",
            "Requirement already satisfied: py-multihash>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (0.2.3)\n",
            "Requirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (43.0.3)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (2.10.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from hivemind==1.2.0.dev0) (24.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4.6->hivemind==1.2.0.dev0) (1.17.1)\n",
            "Requirement already satisfied: grpcio>=1.68.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.33.2->hivemind==1.2.0.dev0) (1.68.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.33.2->hivemind==1.2.0.dev0) (75.1.0)\n",
            "Requirement already satisfied: base58<2.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from py-multihash>=0.2.3->hivemind==1.2.0.dev0) (1.0.3)\n",
            "Requirement already satisfied: morphys<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from py-multihash>=0.2.3->hivemind==1.2.0.dev0) (1.0)\n",
            "Requirement already satisfied: six<2.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from py-multihash>=0.2.3->hivemind==1.2.0.dev0) (1.16.0)\n",
            "Requirement already satisfied: varint<2.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from py-multihash>=0.2.3->hivemind==1.2.0.dev0) (1.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->hivemind==1.2.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->hivemind==1.2.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->hivemind==1.2.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: netaddr in /usr/local/lib/python3.10/dist-packages (from multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce->hivemind==1.2.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: py-cid in /usr/local/lib/python3.10/dist-packages (from multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce->hivemind==1.2.0.dev0) (0.3.0)\n",
            "Requirement already satisfied: py-multicodec>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce->hivemind==1.2.0.dev0) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4.6->hivemind==1.2.0.dev0) (2.22)\n",
            "Requirement already satisfied: py-multibase<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py-cid->multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce->hivemind==1.2.0.dev0) (1.0.3)\n",
            "Requirement already satisfied: python-baseconv<2.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from py-multibase<2.0.0,>=1.0.0->py-cid->multiaddr@ git+https://github.com/multiformats/py-multiaddr.git@e01dbd38f2c0464c0f78b556691d655265018cce->hivemind==1.2.0.dev0) (1.2.2)\n",
            "Using cached protobuf-5.27.5-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.1\n",
            "    Uninstalling protobuf-5.29.1:\n",
            "      Successfully uninstalled protobuf-5.29.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.5 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.27.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.27.5\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/learning-at-home/hivemind.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE516gKxgA6m",
        "outputId": "1682b95b-fe29-4ce4-d1bd-bf601be3563e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.10.26)\n",
            "Collecting wandb\n",
            "  Using cached wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (5.27.5)\n",
            "Collecting protobuf\n",
            "  Using cached protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Using cached wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
            "Using cached protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Installing collected packages: protobuf, wandb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.27.5\n",
            "    Uninstalling protobuf-5.27.5:\n",
            "      Successfully uninstalled protobuf-5.27.5\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.10.26\n",
            "    Uninstalling wandb-0.10.26:\n",
            "      Successfully uninstalled wandb-0.10.26\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hivemind 1.2.0.dev0 requires protobuf<5.28.0,>=3.12.2, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.29.1 wandb-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade wandb protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM_dui0pf7U4"
      },
      "source": [
        "fd946c58a40e64fa7ef780fb501955d3d74d9971"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z2uK2oVHvkP",
        "outputId": "bfa42f7f-8e6b-404c-ef76-7fc126771ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 16:31:57.507110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 16:31:57.526528: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 16:31:57.532409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 16:31:57.547037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 16:31:58.582724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Dec 11 16:32:01.143 [\u001b[1m\u001b[34mINFO\u001b[0m] Found 1 initial peers: ['/dns4/0.tcp.jp.ngrok.io/tcp/12810/p2p/12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2']\n",
            "Dec 11 16:32:01.144 [\u001b[1m\u001b[34mINFO\u001b[0m] Training/evaluation parameters:\n",
            "AlbertTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "clamp_value=10000.0,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=2,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O2,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.00176,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=1000000000000000000000000000000,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "seq_length=512,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "total_steps=125000,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=5000,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "config.json: 100% 685/685 [00:00<00:00, 4.73MB/s]\n",
            "Dec 11 16:32:01.826 [\u001b[1m\u001b[34mINFO\u001b[0m] loading configuration file config.json from cache at data/models--albert-large-v2/snapshots/dfed3a5ef4499fb3351c4ebbcf487375d1e942c8/config.json\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file spiece.model\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer.json\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file added_tokens.json\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file special_tokens_map.json\n",
            "Dec 11 16:32:01.827 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer_config.json\n",
            "Dec 11 16:32:01.881 [\u001b[1m\u001b[34mINFO\u001b[0m] Checkpoint dir outputs, contents []\n",
            "Dec 11 16:32:01.881 [\u001b[1m\u001b[34mINFO\u001b[0m] Training from scratch\n",
            "Dec 11 16:32:02.188 [\u001b[1m\u001b[34mINFO\u001b[0m] You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Dec 11 16:32:07.273 [\u001b[1m\u001b[34mINFO\u001b[0m] Running a DHT instance. To connect other peers to this one, use \u001b[1m\u001b[34m--initial_peers /ip4/127.0.0.1/tcp/34021/p2p/12D3KooWGuNwaXk7gtRsy5V6kZW2fCJ3VfWX2Vk7miJxN6DNZBwQ\u001b[0m\n",
            "Dec 11 16:32:07.274 [\u001b[1m\u001b[34mINFO\u001b[0m] Full list of visible multiaddresses: /ip4/127.0.0.1/tcp/34021/p2p/12D3KooWGuNwaXk7gtRsy5V6kZW2fCJ3VfWX2Vk7miJxN6DNZBwQ /ip4/172.28.0.12/tcp/34021/p2p/12D3KooWGuNwaXk7gtRsy5V6kZW2fCJ3VfWX2Vk7miJxN6DNZBwQ\n",
            "[<Multiaddr /ip4/127.0.0.1/tcp/34021/p2p/12D3KooWGuNwaXk7gtRsy5V6kZW2fCJ3VfWX2Vk7miJxN6DNZBwQ>, <Multiaddr /ip4/172.28.0.12/tcp/34021/p2p/12D3KooWGuNwaXk7gtRsy5V6kZW2fCJ3VfWX2Vk7miJxN6DNZBwQ>]\n",
            "[34021, 34021]\n",
            "Dec 11 16:32:07.281 [\u001b[1m\u001b[34mINFO\u001b[0m] Opening tunnel named: tcp-34021-e05aa995-82ae-4d1d-902f-bf08800498a9\n",
            "Dec 11 16:32:11.366 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "Dec 11 16:32:11.366 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "Dec 11 16:32:11.367 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "Dec 11 16:32:11.381 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "Dec 11 16:32:11.420 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "Dec 11 16:32:11.420 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "Dec 11 16:32:11.422 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=start pg=/api/tunnels id=a18bf1edff0db1d0\n",
            "Dec 11 16:32:11.422 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=end pg=/api/tunnels id=a18bf1edff0db1d0 status=200 dur=220.07µs\n",
            "Dec 11 16:32:11.422 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=start pg=/api/tunnels id=5e94141287b16d23\n",
            "Dec 11 16:32:11.423 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=end pg=/api/tunnels id=5e94141287b16d23 status=200 dur=70.703µs\n",
            "Dec 11 16:32:11.424 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=start pg=/api/tunnels id=9506c2621fe1be07\n",
            "Dec 11 16:32:11.434 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=tcp-34021-e05aa995-82ae-4d1d-902f-bf08800498a9 addr=//localhost:34021 url=tcp://0.tcp.ap.ngrok.io:13972\n",
            "Dec 11 16:32:11.434 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T16:32:11+0000 lvl=info msg=end pg=/api/tunnels id=9506c2621fe1be07 status=201 dur=9.586726ms\n",
            "Dec 11 16:32:11.434 [\u001b[1m\u001b[34mINFO\u001b[0m] Ngrok tunnel created: tcp://0.tcp.ap.ngrok.io:13972\n",
            "Dec 11 16:32:11.434 [\u001b[1m\u001b[34mINFO\u001b[0m] DHT is now accessible via: tcp://0.tcp.ap.ngrok.io:13972\n",
            "Dec 11 16:32:11.602 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 864 samples for epoch #2 from 1 peers. ETA 266.77 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:32:11.661 [\u001b[1m\u001b[34mINFO\u001b[0m] Initializing optimizer manually since it has no tensors in state dict. To override this, provide initialize_optimizer=False\n",
            "/content/./run_trainer.py:345: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TrainerWithIndependentShuffling.__init__`. Use `processing_class` instead.\n",
            "  trainer = TrainerWithIndependentShuffling(\n",
            "Dec 11 16:32:13.502 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mtransformers.trainer.__init__:649\u001b[0m] max_steps is given, it will override any value given in num_train_epochs\n",
            "Dec 11 16:32:13.502 [\u001b[1m\u001b[34mINFO\u001b[0m] Using auto half precision backend\n",
            "training_args.do_train = True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2066: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "Dec 11 16:32:13.803 [\u001b[1m\u001b[34mINFO\u001b[0m] The following columns in the training set don't have a corresponding argument in `AlbertForPreTraining.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `AlbertForPreTraining.forward`,  you can safely ignore this message.\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m] ***** Running training *****\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m]   Num examples = 711,735\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m]   Num Epochs = 5,620,061,371,070,172,086,279,183\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m]   Instantaneous batch size per device = 2\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m]   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "Dec 11 16:32:13.906 [\u001b[1m\u001b[34mINFO\u001b[0m]   Gradient Accumulation steps = 2\n",
            "Dec 11 16:32:13.907 [\u001b[1m\u001b[34mINFO\u001b[0m]   Total optimization steps = 1,000,000,000,000,000,000,000,000,000,000\n",
            "Dec 11 16:32:13.907 [\u001b[1m\u001b[34mINFO\u001b[0m]   Number of trainable parameters = 17,847,474\n",
            "Dec 11 16:32:13.912 [\u001b[1m\u001b[34mINFO\u001b[0m] Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241211_163226-zplo6bu2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutputs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mortis-new-york-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mortis-new-york-university/huggingface/runs/zplo6bu2\u001b[0m\n",
            "Dec 11 16:32:27.700 [\u001b[1m\u001b[34mINFO\u001b[0m] Loading state from peers\n",
            "Dec 11 16:32:27.827 [\u001b[1m\u001b[34mINFO\u001b[0m] Downloading parameters from peer 12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2\n",
            "Dec 11 16:33:00.249 [\u001b[1m\u001b[34mINFO\u001b[0m] Finished downloading state in 32.422s from 12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2\n",
            "Dec 11 16:33:00.563 [\u001b[1m\u001b[34mINFO\u001b[0m] Downloading parameters from peer 12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2\n",
            "Dec 11 16:33:11.486 [\u001b[1m\u001b[34mINFO\u001b[0m] Finished downloading state in 10.923s from 12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2\n",
            "Dec 11 16:33:11.873 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1560 samples for epoch #2 from 1 peers. ETA 225.50 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:33:12.583 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #2\n",
            "Dec 11 16:33:12.583 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 4 samples\n",
            "Dec 11 16:33:12.583 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 7.795 samples/sec\n",
            "Dec 11 16:33:12.583 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.04380\n",
            "Dec 11 16:33:41.996 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2224 samples for epoch #2 from 2 peers. ETA 85.44 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:34:12.140 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2896 samples for epoch #2 from 2 peers. ETA 50.06 sec (refresh in 20.02 sec)\n",
            "Dec 11 16:34:32.279 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3332 samples for epoch #2 from 2 peers. ETA 35.57 sec (refresh in 14.23 sec)\n",
            "Dec 11 16:34:46.619 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3660 samples for epoch #2 from 2 peers. ETA 18.69 sec (refresh in 7.47 sec)\n",
            "Dec 11 16:34:54.212 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3836 samples for epoch #2 from 2 peers. ETA 10.95 sec (refresh in 4.38 sec)\n",
            "Dec 11 16:34:58.704 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3948 samples for epoch #2 from 2 peers. ETA 5.84 sec (refresh in 2.34 sec)\n",
            "Dec 11 16:34:59.765 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:35:01.153 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3996 samples for epoch #2 from 2 peers. ETA 3.72 sec (refresh in 1.49 sec)\n",
            "Dec 11 16:35:02.758 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4032 samples for epoch #2 from 2 peers. ETA 2.39 sec (refresh in 0.96 sec)\n",
            "Dec 11 16:35:03.838 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #2 from 2 peers. ETA 1.31 sec (refresh in 0.52 sec)\n",
            "Dec 11 16:35:04.474 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #2 from 2 peers. ETA 0.55 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:35:05.087 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #2 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:35:05.087 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #2\n",
            "Dec 11 16:35:05.114 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 3\n",
            "Dec 11 16:35:05.119 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #3\n",
            "Dec 11 16:35:05.119 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 1300 samples\n",
            "Dec 11 16:35:05.119 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.298 samples/sec\n",
            "Dec 11 16:35:05.119 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01519\n",
            "Dec 11 16:35:05.705 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #3 from 2 peers. ETA 177.40 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:35:13.246 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:35:20.832 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:35:20.853 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:35:35.819 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 688 samples for epoch #3 from 2 peers. ETA 146.83 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:36:05.305 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-500\n",
            "Dec 11 16:36:05.307 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-500/config.json\n",
            "Dec 11 16:36:05.451 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-500/model.safetensors\n",
            "Dec 11 16:36:05.452 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n",
            "Dec 11 16:36:05.453 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n",
            "Dec 11 16:36:05.932 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1384 samples for epoch #3 from 2 peers. ETA 109.91 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:36:36.045 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2048 samples for epoch #3 from 2 peers. ETA 89.48 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:37:06.159 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2724 samples for epoch #3 from 2 peers. ETA 57.70 sec (refresh in 23.08 sec)\n",
            "Dec 11 16:37:29.355 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3248 samples for epoch #3 from 2 peers. ETA 36.81 sec (refresh in 14.73 sec)\n",
            "Dec 11 16:37:44.196 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3576 samples for epoch #3 from 2 peers. ETA 22.95 sec (refresh in 9.18 sec)\n",
            "Dec 11 16:37:53.493 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3804 samples for epoch #3 from 2 peers. ETA 11.48 sec (refresh in 4.59 sec)\n",
            "Dec 11 16:37:58.197 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3920 samples for epoch #3 from 2 peers. ETA 6.84 sec (refresh in 2.74 sec)\n",
            "Dec 11 16:38:00.462 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:38:01.046 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3988 samples for epoch #3 from 2 peers. ETA 4.25 sec (refresh in 1.70 sec)\n",
            "Dec 11 16:38:02.861 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #3 from 2 peers. ETA 2.23 sec (refresh in 0.89 sec)\n",
            "Dec 11 16:38:03.868 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #3 from 2 peers. ETA 1.34 sec (refresh in 0.54 sec)\n",
            "Dec 11 16:38:04.656 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #3 from 2 peers. ETA 0.67 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:38:05.274 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4092 samples for epoch #3 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:38:05.468 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #3\n",
            "Dec 11 16:38:05.494 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 4\n",
            "Dec 11 16:38:05.499 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #4\n",
            "Dec 11 16:38:05.499 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 3412 samples\n",
            "Dec 11 16:38:05.500 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.067 samples/sec\n",
            "Dec 11 16:38:05.500 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00988\n",
            "Dec 11 16:38:05.888 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #4 from 2 peers. ETA 167.39 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:38:11.734 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:38:18.804 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:38:19.121 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:38:36.002 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 640 samples for epoch #4 from 2 peers. ETA 166.63 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:38:55.903 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-1000\n",
            "Dec 11 16:38:55.905 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-1000/config.json\n",
            "Dec 11 16:38:56.045 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-1000/model.safetensors\n",
            "Dec 11 16:38:56.046 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n",
            "Dec 11 16:38:56.047 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n",
            "Dec 11 16:39:06.117 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1304 samples for epoch #4 from 2 peers. ETA 125.44 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:39:36.251 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2016 samples for epoch #4 from 2 peers. ETA 89.11 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:40:06.368 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2708 samples for epoch #4 from 2 peers. ETA 60.82 sec (refresh in 24.33 sec)\n",
            "Dec 11 16:40:30.808 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3268 samples for epoch #4 from 2 peers. ETA 36.06 sec (refresh in 14.42 sec)\n",
            "Dec 11 16:40:45.368 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3592 samples for epoch #4 from 2 peers. ETA 22.10 sec (refresh in 8.84 sec)\n",
            "Dec 11 16:40:54.325 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3784 samples for epoch #4 from 2 peers. ETA 13.65 sec (refresh in 5.46 sec)\n",
            "Dec 11 16:40:59.901 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3912 samples for epoch #4 from 2 peers. ETA 7.65 sec (refresh in 3.06 sec)\n",
            "Dec 11 16:41:02.565 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:41:03.078 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3980 samples for epoch #4 from 2 peers. ETA 4.75 sec (refresh in 1.90 sec)\n",
            "Dec 11 16:41:05.093 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #4 from 2 peers. ETA 2.78 sec (refresh in 1.11 sec)\n",
            "Dec 11 16:41:06.321 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #4 from 2 peers. ETA 1.47 sec (refresh in 0.59 sec)\n",
            "Dec 11 16:41:07.023 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #4 from 2 peers. ETA 0.64 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:41:07.639 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #4 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:41:07.973 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #4\n",
            "Dec 11 16:41:07.999 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 5\n",
            "Dec 11 16:41:08.005 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #5\n",
            "Dec 11 16:41:08.005 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 5532 samples\n",
            "Dec 11 16:41:08.005 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.509 samples/sec\n",
            "Dec 11 16:41:08.006 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01760\n",
            "Dec 11 16:41:08.282 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #5 from 2 peers. ETA 175.45 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:41:16.263 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:41:24.689 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:41:24.840 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:41:38.399 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 704 samples for epoch #5 from 2 peers. ETA 136.89 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:41:46.479 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-1500\n",
            "Dec 11 16:41:46.480 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-1500/config.json\n",
            "Dec 11 16:41:46.619 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-1500/model.safetensors\n",
            "Dec 11 16:41:46.620 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\n",
            "Dec 11 16:41:46.621 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\n",
            "Dec 11 16:41:46.848 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-500] due to args.save_total_limit\n",
            "Dec 11 16:42:08.514 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1396 samples for epoch #5 from 2 peers. ETA 120.04 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:42:38.629 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2096 samples for epoch #5 from 2 peers. ETA 83.83 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:43:08.747 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2780 samples for epoch #5 from 2 peers. ETA 52.21 sec (refresh in 20.88 sec)\n",
            "Dec 11 16:43:29.747 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3252 samples for epoch #5 from 2 peers. ETA 38.41 sec (refresh in 15.36 sec)\n",
            "Dec 11 16:43:45.229 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3584 samples for epoch #5 from 2 peers. ETA 22.89 sec (refresh in 9.15 sec)\n",
            "Dec 11 16:43:54.606 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3796 samples for epoch #5 from 2 peers. ETA 12.70 sec (refresh in 5.08 sec)\n",
            "Dec 11 16:43:59.803 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3920 samples for epoch #5 from 2 peers. ETA 6.82 sec (refresh in 2.73 sec)\n",
            "Dec 11 16:44:01.679 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:44:02.658 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #5 from 2 peers. ETA 4.37 sec (refresh in 1.75 sec)\n",
            "Dec 11 16:44:04.521 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4020 samples for epoch #5 from 2 peers. ETA 2.71 sec (refresh in 1.08 sec)\n",
            "Dec 11 16:44:05.719 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #5 from 2 peers. ETA 1.68 sec (refresh in 0.67 sec)\n",
            "Dec 11 16:44:06.507 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4072 samples for epoch #5 from 2 peers. ETA 0.64 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:44:07.129 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #5 from 2 peers. ETA 0.11 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:44:07.306 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #5\n",
            "Dec 11 16:44:07.334 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 6\n",
            "Dec 11 16:44:07.341 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #6\n",
            "Dec 11 16:44:07.341 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 7620 samples\n",
            "Dec 11 16:44:07.342 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.353 samples/sec\n",
            "Dec 11 16:44:07.342 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01209\n",
            "Dec 11 16:44:07.802 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #6 from 2 peers. ETA 178.23 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:44:15.830 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:44:24.206 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:44:24.313 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:44:37.920 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 664 samples for epoch #6 from 2 peers. ETA 152.87 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:44:39.366 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-2000\n",
            "Dec 11 16:44:39.367 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-2000/config.json\n",
            "Dec 11 16:44:39.507 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-2000/model.safetensors\n",
            "Dec 11 16:44:39.508 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\n",
            "Dec 11 16:44:39.509 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n",
            "Dec 11 16:44:39.745 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-1000] due to args.save_total_limit\n",
            "Dec 11 16:45:08.038 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1352 samples for epoch #6 from 2 peers. ETA 123.20 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:45:38.156 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2028 samples for epoch #6 from 2 peers. ETA 90.38 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:46:08.278 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2696 samples for epoch #6 from 2 peers. ETA 61.18 sec (refresh in 24.47 sec)\n",
            "Dec 11 16:46:32.865 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3260 samples for epoch #6 from 2 peers. ETA 35.45 sec (refresh in 14.18 sec)\n",
            "Dec 11 16:46:47.165 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3576 samples for epoch #6 from 2 peers. ETA 23.64 sec (refresh in 9.46 sec)\n",
            "Dec 11 16:46:56.737 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3796 samples for epoch #6 from 2 peers. ETA 12.87 sec (refresh in 5.15 sec)\n",
            "Dec 11 16:47:02.001 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3920 samples for epoch #6 from 2 peers. ETA 7.25 sec (refresh in 2.90 sec)\n",
            "Dec 11 16:47:04.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:47:05.017 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3988 samples for epoch #6 from 2 peers. ETA 4.64 sec (refresh in 1.86 sec)\n",
            "Dec 11 16:47:07.150 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4032 samples for epoch #6 from 2 peers. ETA 2.07 sec (refresh in 0.83 sec)\n",
            "Dec 11 16:47:08.096 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #6 from 2 peers. ETA 1.36 sec (refresh in 0.54 sec)\n",
            "Dec 11 16:47:08.755 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #6 from 2 peers. ETA 0.73 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:47:09.370 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #6 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:47:09.824 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #6\n",
            "Dec 11 16:47:09.850 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 7\n",
            "Dec 11 16:47:09.855 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #7\n",
            "Dec 11 16:47:09.855 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 9732 samples\n",
            "Dec 11 16:47:09.855 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.103 samples/sec\n",
            "Dec 11 16:47:09.856 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01150\n",
            "Dec 11 16:47:09.985 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #7 from 2 peers. ETA 176.57 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:47:19.331 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:47:27.544 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:47:27.569 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:47:31.843 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-2500\n",
            "Dec 11 16:47:31.844 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-2500/config.json\n",
            "Dec 11 16:47:31.984 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-2500/model.safetensors\n",
            "Dec 11 16:47:31.985 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\n",
            "Dec 11 16:47:31.985 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\n",
            "Dec 11 16:47:32.241 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-1500] due to args.save_total_limit\n",
            "Dec 11 16:47:40.101 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 636 samples for epoch #7 from 2 peers. ETA 158.99 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:48:10.217 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1340 samples for epoch #7 from 2 peers. ETA 109.30 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:48:40.333 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2016 samples for epoch #7 from 2 peers. ETA 90.18 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:49:10.450 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2704 samples for epoch #7 from 2 peers. ETA 59.09 sec (refresh in 23.64 sec)\n",
            "Dec 11 16:49:34.201 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3276 samples for epoch #7 from 2 peers. ETA 34.58 sec (refresh in 13.83 sec)\n",
            "Dec 11 16:49:48.148 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3608 samples for epoch #7 from 2 peers. ETA 19.95 sec (refresh in 7.98 sec)\n",
            "Dec 11 16:49:56.277 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3796 samples for epoch #7 from 2 peers. ETA 12.31 sec (refresh in 4.92 sec)\n",
            "Dec 11 16:50:01.327 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #7 from 2 peers. ETA 7.61 sec (refresh in 3.04 sec)\n",
            "Dec 11 16:50:04.080 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:50:04.487 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #7 from 2 peers. ETA 4.30 sec (refresh in 1.72 sec)\n",
            "Dec 11 16:50:06.327 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4020 samples for epoch #7 from 2 peers. ETA 2.90 sec (refresh in 1.16 sec)\n",
            "Dec 11 16:50:07.607 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4060 samples for epoch #7 from 2 peers. ETA 1.34 sec (refresh in 0.53 sec)\n",
            "Dec 11 16:50:08.270 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #7 from 2 peers. ETA 0.62 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:50:08.890 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #7 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:50:09.090 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #7\n",
            "Dec 11 16:50:09.116 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 8\n",
            "Dec 11 16:50:09.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #8\n",
            "Dec 11 16:50:09.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 11808 samples\n",
            "Dec 11 16:50:09.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.441 samples/sec\n",
            "Dec 11 16:50:09.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01316\n",
            "Dec 11 16:50:09.505 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #8 from 2 peers. ETA 176.60 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:50:15.563 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:50:22.210 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-3000\n",
            "Dec 11 16:50:22.212 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-3000/config.json\n",
            "Dec 11 16:50:22.362 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-3000/model.safetensors\n",
            "Dec 11 16:50:22.363 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\n",
            "Dec 11 16:50:22.363 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\n",
            "Dec 11 16:50:22.630 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-2000] due to args.save_total_limit\n",
            "Dec 11 16:50:23.053 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:50:23.102 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:50:39.621 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 716 samples for epoch #8 from 2 peers. ETA 145.59 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:51:09.735 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1372 samples for epoch #8 from 2 peers. ETA 127.32 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:51:39.851 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2092 samples for epoch #8 from 2 peers. ETA 86.45 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:52:09.989 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2772 samples for epoch #8 from 2 peers. ETA 55.82 sec (refresh in 22.33 sec)\n",
            "Dec 11 16:52:32.434 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3284 samples for epoch #8 from 2 peers. ETA 32.88 sec (refresh in 13.15 sec)\n",
            "Dec 11 16:52:45.702 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3580 samples for epoch #8 from 2 peers. ETA 21.02 sec (refresh in 8.41 sec)\n",
            "Dec 11 16:52:54.225 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3776 samples for epoch #8 from 2 peers. ETA 13.85 sec (refresh in 5.54 sec)\n",
            "Dec 11 16:52:59.881 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #8 from 2 peers. ETA 8.12 sec (refresh in 3.25 sec)\n",
            "Dec 11 16:53:03.020 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:53:03.245 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3988 samples for epoch #8 from 2 peers. ETA 4.49 sec (refresh in 1.80 sec)\n",
            "Dec 11 16:53:05.155 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4032 samples for epoch #8 from 2 peers. ETA 2.51 sec (refresh in 1.01 sec)\n",
            "Dec 11 16:53:06.276 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #8 from 2 peers. ETA 1.30 sec (refresh in 0.52 sec)\n",
            "Dec 11 16:53:06.911 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #8 from 2 peers. ETA 0.87 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:53:07.549 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #8 from 2 peers. ETA 0.19 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:53:08.000 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #8\n",
            "Dec 11 16:53:08.026 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 9\n",
            "Dec 11 16:53:08.033 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #9\n",
            "Dec 11 16:53:08.033 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 13904 samples\n",
            "Dec 11 16:53:08.033 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.371 samples/sec\n",
            "Dec 11 16:53:08.033 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01374\n",
            "Dec 11 16:53:08.174 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #9 from 2 peers. ETA 185.93 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:53:13.214 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-3500\n",
            "Dec 11 16:53:13.216 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-3500/config.json\n",
            "Dec 11 16:53:13.366 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-3500/model.safetensors\n",
            "Dec 11 16:53:13.367 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\n",
            "Dec 11 16:53:13.367 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\n",
            "Dec 11 16:53:13.642 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-2500] due to args.save_total_limit\n",
            "Dec 11 16:53:16.434 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:53:23.946 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:53:24.058 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:53:38.290 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 680 samples for epoch #9 from 2 peers. ETA 153.91 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:54:08.406 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1356 samples for epoch #9 from 2 peers. ETA 120.66 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:54:38.524 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2036 samples for epoch #9 from 2 peers. ETA 87.93 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:55:08.643 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2720 samples for epoch #9 from 2 peers. ETA 59.37 sec (refresh in 23.75 sec)\n",
            "Dec 11 16:55:32.508 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3248 samples for epoch #9 from 2 peers. ETA 36.54 sec (refresh in 14.61 sec)\n",
            "Dec 11 16:55:47.244 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3580 samples for epoch #9 from 2 peers. ETA 22.54 sec (refresh in 9.02 sec)\n",
            "Dec 11 16:55:56.378 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3780 samples for epoch #9 from 2 peers. ETA 13.79 sec (refresh in 5.52 sec)\n",
            "Dec 11 16:56:02.028 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #9 from 2 peers. ETA 7.75 sec (refresh in 3.10 sec)\n",
            "Dec 11 16:56:04.786 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:56:05.250 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #9 from 2 peers. ETA 4.34 sec (refresh in 1.73 sec)\n",
            "Dec 11 16:56:07.102 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4032 samples for epoch #9 from 2 peers. ETA 2.54 sec (refresh in 1.02 sec)\n",
            "Dec 11 16:56:08.238 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #9 from 2 peers. ETA 1.43 sec (refresh in 0.57 sec)\n",
            "Dec 11 16:56:08.463 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-4000\n",
            "Dec 11 16:56:08.464 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-4000/config.json\n",
            "Dec 11 16:56:08.608 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-4000/model.safetensors\n",
            "Dec 11 16:56:08.609 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-4000/tokenizer_config.json\n",
            "Dec 11 16:56:08.609 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-4000/special_tokens_map.json\n",
            "Dec 11 16:56:08.899 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-3000] due to args.save_total_limit\n",
            "Dec 11 16:56:09.025 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4072 samples for epoch #9 from 2 peers. ETA 0.60 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:56:09.645 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #9 from 2 peers. ETA 0.22 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:56:09.896 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #9\n",
            "Dec 11 16:56:09.922 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 10\n",
            "Dec 11 16:56:09.929 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #10\n",
            "Dec 11 16:56:09.929 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 15976 samples\n",
            "Dec 11 16:56:09.930 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 10.360 samples/sec\n",
            "Dec 11 16:56:09.930 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00674\n",
            "Dec 11 16:56:10.504 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #10 from 2 peers. ETA 181.60 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:56:16.518 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:56:25.213 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:56:25.307 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:56:40.621 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 656 samples for epoch #10 from 2 peers. ETA 150.19 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:57:10.738 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1352 samples for epoch #10 from 2 peers. ETA 116.84 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:57:40.855 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2052 samples for epoch #10 from 2 peers. ETA 91.05 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:58:10.970 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2736 samples for epoch #10 from 2 peers. ETA 56.88 sec (refresh in 22.75 sec)\n",
            "Dec 11 16:58:33.841 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3260 samples for epoch #10 from 2 peers. ETA 33.87 sec (refresh in 13.55 sec)\n",
            "Dec 11 16:58:47.506 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3564 samples for epoch #10 from 2 peers. ETA 25.10 sec (refresh in 10.04 sec)\n",
            "Dec 11 16:58:57.661 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3808 samples for epoch #10 from 2 peers. ETA 11.60 sec (refresh in 4.64 sec)\n",
            "Dec 11 16:59:02.418 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3916 samples for epoch #10 from 2 peers. ETA 6.84 sec (refresh in 2.73 sec)\n",
            "Dec 11 16:59:03.037 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-4500\n",
            "Dec 11 16:59:03.039 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-4500/config.json\n",
            "Dec 11 16:59:03.184 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-4500/model.safetensors\n",
            "Dec 11 16:59:03.185 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-4500/tokenizer_config.json\n",
            "Dec 11 16:59:03.185 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-4500/special_tokens_map.json\n",
            "Dec 11 16:59:03.488 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-3500] due to args.save_total_limit\n",
            "Dec 11 16:59:04.693 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 16:59:05.268 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3976 samples for epoch #10 from 2 peers. ETA 5.07 sec (refresh in 2.03 sec)\n",
            "Dec 11 16:59:07.412 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4020 samples for epoch #10 from 2 peers. ETA 3.06 sec (refresh in 1.22 sec)\n",
            "Dec 11 16:59:08.750 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #10 from 2 peers. ETA 1.73 sec (refresh in 0.69 sec)\n",
            "Dec 11 16:59:09.559 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4064 samples for epoch #10 from 2 peers. ETA 0.77 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:59:10.175 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #10 from 2 peers. ETA 0.39 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:59:10.791 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4092 samples for epoch #10 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 16:59:10.888 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #10\n",
            "Dec 11 16:59:10.914 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 11\n",
            "Dec 11 16:59:10.920 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #11\n",
            "Dec 11 16:59:10.920 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 18036 samples\n",
            "Dec 11 16:59:10.920 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 10.860 samples/sec\n",
            "Dec 11 16:59:10.920 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01302\n",
            "Dec 11 16:59:11.432 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #11 from 2 peers. ETA 190.50 sec (refresh in 30.00 sec)\n",
            "Dec 11 16:59:20.164 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 16:59:28.662 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 16:59:28.726 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 16:59:41.550 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 676 samples for epoch #11 from 2 peers. ETA 152.89 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:00:11.665 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1344 samples for epoch #11 from 2 peers. ETA 118.47 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:00:41.778 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2048 samples for epoch #11 from 2 peers. ETA 88.30 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:01:11.895 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2744 samples for epoch #11 from 2 peers. ETA 58.59 sec (refresh in 23.44 sec)\n",
            "Dec 11 17:01:35.477 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3260 samples for epoch #11 from 2 peers. ETA 36.49 sec (refresh in 14.60 sec)\n",
            "Dec 11 17:01:50.198 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3608 samples for epoch #11 from 2 peers. ETA 20.62 sec (refresh in 8.25 sec)\n",
            "Dec 11 17:01:58.286 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-5000\n",
            "Dec 11 17:01:58.288 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-5000/config.json\n",
            "Dec 11 17:01:58.429 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-5000/model.safetensors\n",
            "Dec 11 17:01:58.430 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-5000/tokenizer_config.json\n",
            "Dec 11 17:01:58.431 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-5000/special_tokens_map.json\n",
            "Dec 11 17:01:58.583 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3792 samples for epoch #11 from 2 peers. ETA 13.47 sec (refresh in 5.39 sec)\n",
            "Dec 11 17:01:58.757 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-4000] due to args.save_total_limit\n",
            "Dec 11 17:02:04.083 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3912 samples for epoch #11 from 2 peers. ETA 7.83 sec (refresh in 3.13 sec)\n",
            "Dec 11 17:02:07.053 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:02:07.325 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #11 from 2 peers. ETA 4.13 sec (refresh in 1.65 sec)\n",
            "Dec 11 17:02:09.090 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4024 samples for epoch #11 from 2 peers. ETA 2.89 sec (refresh in 1.16 sec)\n",
            "Dec 11 17:02:10.359 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #11 from 2 peers. ETA 1.47 sec (refresh in 0.59 sec)\n",
            "Dec 11 17:02:11.062 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #11 from 2 peers. ETA 0.56 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:02:11.675 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4092 samples for epoch #11 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:02:11.876 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #11\n",
            "Dec 11 17:02:11.902 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 12\n",
            "Dec 11 17:02:11.917 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #12\n",
            "Dec 11 17:02:11.917 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 20104 samples\n",
            "Dec 11 17:02:11.917 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.714 samples/sec\n",
            "Dec 11 17:02:11.918 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01225\n",
            "Dec 11 17:02:12.292 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #12 from 2 peers. ETA 176.90 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:02:19.703 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:02:27.027 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:02:27.380 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:02:42.406 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 684 samples for epoch #12 from 2 peers. ETA 151.10 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:03:12.518 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1364 samples for epoch #12 from 2 peers. ETA 119.48 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:03:42.631 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2084 samples for epoch #12 from 2 peers. ETA 86.66 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:04:12.744 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2792 samples for epoch #12 from 2 peers. ETA 52.63 sec (refresh in 21.05 sec)\n",
            "Dec 11 17:04:33.911 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3268 samples for epoch #12 from 2 peers. ETA 36.17 sec (refresh in 14.47 sec)\n",
            "Dec 11 17:04:48.496 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3604 samples for epoch #12 from 2 peers. ETA 21.63 sec (refresh in 8.65 sec)\n",
            "Dec 11 17:04:50.845 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-5500\n",
            "Dec 11 17:04:50.847 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-5500/config.json\n",
            "Dec 11 17:04:50.988 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-5500/model.safetensors\n",
            "Dec 11 17:04:50.989 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-5500/tokenizer_config.json\n",
            "Dec 11 17:04:50.989 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-5500/special_tokens_map.json\n",
            "Dec 11 17:04:51.308 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-4500] due to args.save_total_limit\n",
            "Dec 11 17:04:57.259 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3792 samples for epoch #12 from 2 peers. ETA 12.77 sec (refresh in 5.11 sec)\n",
            "Dec 11 17:05:02.480 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #12 from 2 peers. ETA 7.75 sec (refresh in 3.10 sec)\n",
            "Dec 11 17:05:05.471 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:05:05.692 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3988 samples for epoch #12 from 2 peers. ETA 4.10 sec (refresh in 1.64 sec)\n",
            "Dec 11 17:05:07.447 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #12 from 2 peers. ETA 2.71 sec (refresh in 1.08 sec)\n",
            "Dec 11 17:05:08.644 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #12 from 2 peers. ETA 1.54 sec (refresh in 0.62 sec)\n",
            "Dec 11 17:05:09.375 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4064 samples for epoch #12 from 2 peers. ETA 0.94 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:05:09.990 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #12 from 2 peers. ETA 0.11 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:05:10.432 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #12\n",
            "Dec 11 17:05:10.461 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 13\n",
            "Dec 11 17:05:10.467 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #13\n",
            "Dec 11 17:05:10.467 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 22164 samples\n",
            "Dec 11 17:05:10.467 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 10.778 samples/sec\n",
            "Dec 11 17:05:10.468 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00235\n",
            "Dec 11 17:05:10.636 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #13 from 2 peers. ETA 180.41 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:05:19.188 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:05:27.257 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:05:27.524 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:05:40.753 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 656 samples for epoch #13 from 2 peers. ETA 147.86 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:06:10.876 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1332 samples for epoch #13 from 2 peers. ETA 118.82 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:06:40.994 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2024 samples for epoch #13 from 2 peers. ETA 94.37 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:07:11.110 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2720 samples for epoch #13 from 2 peers. ETA 58.99 sec (refresh in 23.60 sec)\n",
            "Dec 11 17:07:34.820 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3252 samples for epoch #13 from 2 peers. ETA 36.57 sec (refresh in 14.63 sec)\n",
            "Dec 11 17:07:46.468 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-6000\n",
            "Dec 11 17:07:46.470 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-6000/config.json\n",
            "Dec 11 17:07:46.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-6000/model.safetensors\n",
            "Dec 11 17:07:46.611 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-6000/tokenizer_config.json\n",
            "Dec 11 17:07:46.612 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-6000/special_tokens_map.json\n",
            "Dec 11 17:07:46.945 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-5000] due to args.save_total_limit\n",
            "Dec 11 17:07:49.561 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3604 samples for epoch #13 from 2 peers. ETA 21.22 sec (refresh in 8.49 sec)\n",
            "Dec 11 17:07:58.162 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3776 samples for epoch #13 from 2 peers. ETA 14.26 sec (refresh in 5.71 sec)\n",
            "Dec 11 17:08:03.981 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3888 samples for epoch #13 from 2 peers. ETA 9.67 sec (refresh in 3.87 sec)\n",
            "Dec 11 17:08:07.964 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3968 samples for epoch #13 from 2 peers. ETA 5.80 sec (refresh in 2.32 sec)\n",
            "Dec 11 17:08:09.102 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:08:10.405 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #13 from 2 peers. ETA 2.63 sec (refresh in 1.05 sec)\n",
            "Dec 11 17:08:11.572 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #13 from 2 peers. ETA 1.57 sec (refresh in 0.63 sec)\n",
            "Dec 11 17:08:12.313 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4080 samples for epoch #13 from 2 peers. ETA 0.39 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:08:12.926 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #13 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:08:12.926 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #13\n",
            "Dec 11 17:08:12.952 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 14\n",
            "Dec 11 17:08:12.957 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #14\n",
            "Dec 11 17:08:12.957 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 24212 samples\n",
            "Dec 11 17:08:12.957 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.138 samples/sec\n",
            "Dec 11 17:08:12.957 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00604\n",
            "Dec 11 17:08:13.538 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #14 from 2 peers. ETA 180.64 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:08:20.782 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:08:26.330 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:08:26.463 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:08:43.651 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 692 samples for epoch #14 from 2 peers. ETA 145.51 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:09:13.765 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1364 samples for epoch #14 from 2 peers. ETA 120.95 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:09:43.878 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2052 samples for epoch #14 from 2 peers. ETA 90.08 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:10:14.004 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2760 samples for epoch #14 from 2 peers. ETA 56.17 sec (refresh in 22.47 sec)\n",
            "Dec 11 17:10:36.585 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3264 samples for epoch #14 from 2 peers. ETA 36.57 sec (refresh in 14.63 sec)\n",
            "Dec 11 17:10:40.719 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-6500\n",
            "Dec 11 17:10:40.721 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-6500/config.json\n",
            "Dec 11 17:10:40.862 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-6500/model.safetensors\n",
            "Dec 11 17:10:40.863 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-6500/tokenizer_config.json\n",
            "Dec 11 17:10:40.863 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-6500/special_tokens_map.json\n",
            "Dec 11 17:10:41.203 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-5500] due to args.save_total_limit\n",
            "Dec 11 17:10:51.327 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3616 samples for epoch #14 from 2 peers. ETA 19.55 sec (refresh in 7.82 sec)\n",
            "Dec 11 17:10:59.265 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3788 samples for epoch #14 from 2 peers. ETA 13.34 sec (refresh in 5.33 sec)\n",
            "Dec 11 17:11:04.727 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3920 samples for epoch #14 from 2 peers. ETA 6.87 sec (refresh in 2.75 sec)\n",
            "Dec 11 17:11:06.648 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:11:07.592 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3980 samples for epoch #14 from 2 peers. ETA 4.80 sec (refresh in 1.92 sec)\n",
            "Dec 11 17:11:09.630 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4024 samples for epoch #14 from 2 peers. ETA 2.80 sec (refresh in 1.12 sec)\n",
            "Dec 11 17:11:10.866 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4048 samples for epoch #14 from 2 peers. ETA 1.76 sec (refresh in 0.70 sec)\n",
            "Dec 11 17:11:11.686 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #14 from 2 peers. ETA 0.69 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:11:12.307 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #14 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:11:12.388 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #14\n",
            "Dec 11 17:11:12.413 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 15\n",
            "Dec 11 17:11:12.419 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #15\n",
            "Dec 11 17:11:12.420 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 26296 samples\n",
            "Dec 11 17:11:12.420 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.189 samples/sec\n",
            "Dec 11 17:11:12.420 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00753\n",
            "Dec 11 17:11:12.948 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #15 from 2 peers. ETA 179.96 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:11:20.205 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:11:28.926 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:11:29.240 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:11:43.064 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 660 samples for epoch #15 from 2 peers. ETA 160.34 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:12:13.184 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1316 samples for epoch #15 from 2 peers. ETA 127.06 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:12:43.305 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1996 samples for epoch #15 from 2 peers. ETA 88.49 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:13:13.449 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2692 samples for epoch #15 from 2 peers. ETA 60.82 sec (refresh in 24.33 sec)\n",
            "Dec 11 17:13:35.564 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-7000\n",
            "Dec 11 17:13:35.566 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-7000/config.json\n",
            "Dec 11 17:13:35.709 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-7000/model.safetensors\n",
            "Dec 11 17:13:35.710 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-7000/tokenizer_config.json\n",
            "Dec 11 17:13:35.710 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-7000/special_tokens_map.json\n",
            "Dec 11 17:13:36.075 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-6000] due to args.save_total_limit\n",
            "Dec 11 17:13:37.894 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3240 samples for epoch #15 from 2 peers. ETA 40.45 sec (refresh in 16.18 sec)\n",
            "Dec 11 17:13:54.189 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3624 samples for epoch #15 from 2 peers. ETA 19.34 sec (refresh in 7.74 sec)\n",
            "Dec 11 17:14:02.041 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3800 samples for epoch #15 from 2 peers. ETA 11.90 sec (refresh in 4.76 sec)\n",
            "Dec 11 17:14:06.917 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3920 samples for epoch #15 from 2 peers. ETA 7.19 sec (refresh in 2.88 sec)\n",
            "Dec 11 17:14:09.491 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:14:09.911 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3992 samples for epoch #15 from 2 peers. ETA 4.29 sec (refresh in 1.72 sec)\n",
            "Dec 11 17:14:11.742 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #15 from 2 peers. ETA 2.38 sec (refresh in 0.95 sec)\n",
            "Dec 11 17:14:12.809 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4060 samples for epoch #15 from 2 peers. ETA 1.26 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:14:13.438 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #15 from 2 peers. ETA 0.55 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:14:14.055 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4092 samples for epoch #15 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:14:14.126 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #15\n",
            "Dec 11 17:14:14.152 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 16\n",
            "Dec 11 17:14:14.159 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #16\n",
            "Dec 11 17:14:14.159 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 28380 samples\n",
            "Dec 11 17:14:14.159 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.792 samples/sec\n",
            "Dec 11 17:14:14.159 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 10.99753\n",
            "Dec 11 17:14:14.672 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #16 from 2 peers. ETA 164.99 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:14:20.900 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:14:28.608 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:14:28.897 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:14:44.789 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 660 samples for epoch #16 from 2 peers. ETA 158.63 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:15:14.907 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1376 samples for epoch #16 from 2 peers. ETA 108.43 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:15:45.040 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2032 samples for epoch #16 from 2 peers. ETA 86.56 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:16:15.156 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2724 samples for epoch #16 from 2 peers. ETA 53.02 sec (refresh in 21.21 sec)\n",
            "Dec 11 17:16:27.905 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-7500\n",
            "Dec 11 17:16:27.907 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-7500/config.json\n",
            "Dec 11 17:16:28.050 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-7500/model.safetensors\n",
            "Dec 11 17:16:28.051 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-7500/tokenizer_config.json\n",
            "Dec 11 17:16:28.052 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-7500/special_tokens_map.json\n",
            "Dec 11 17:16:28.422 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-6500] due to args.save_total_limit\n",
            "Dec 11 17:16:36.483 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3200 samples for epoch #16 from 2 peers. ETA 40.22 sec (refresh in 16.09 sec)\n",
            "Dec 11 17:16:52.692 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3584 samples for epoch #16 from 2 peers. ETA 22.44 sec (refresh in 8.98 sec)\n",
            "Dec 11 17:17:01.788 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3804 samples for epoch #16 from 2 peers. ETA 11.51 sec (refresh in 4.60 sec)\n",
            "Dec 11 17:17:06.509 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #16 from 2 peers. ETA 7.28 sec (refresh in 2.91 sec)\n",
            "Dec 11 17:17:08.953 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:17:09.542 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #16 from 2 peers. ETA 4.59 sec (refresh in 1.84 sec)\n",
            "Dec 11 17:17:11.499 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4016 samples for epoch #16 from 2 peers. ETA 3.70 sec (refresh in 1.48 sec)\n",
            "Dec 11 17:17:13.095 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #16 from 2 peers. ETA 1.65 sec (refresh in 0.66 sec)\n",
            "Dec 11 17:17:13.882 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #16 from 2 peers. ETA 0.82 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:17:14.515 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #16 from 2 peers. ETA 0.01 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:17:14.691 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #16\n",
            "Dec 11 17:17:14.718 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 17\n",
            "Dec 11 17:17:14.724 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #17\n",
            "Dec 11 17:17:14.724 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 30456 samples\n",
            "Dec 11 17:17:14.724 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 10.817 samples/sec\n",
            "Dec 11 17:17:14.725 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00161\n",
            "Dec 11 17:17:15.283 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #17 from 2 peers. ETA 189.00 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:17:20.999 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:17:28.740 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:17:28.986 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:17:45.404 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 680 samples for epoch #17 from 2 peers. ETA 157.11 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:18:15.521 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1376 samples for epoch #17 from 2 peers. ETA 124.98 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:18:45.641 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2064 samples for epoch #17 from 2 peers. ETA 90.44 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:19:15.758 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2720 samples for epoch #17 from 2 peers. ETA 60.89 sec (refresh in 24.35 sec)\n",
            "Dec 11 17:19:23.191 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-8000\n",
            "Dec 11 17:19:23.193 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-8000/config.json\n",
            "Dec 11 17:19:23.340 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-8000/model.safetensors\n",
            "Dec 11 17:19:23.341 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-8000/tokenizer_config.json\n",
            "Dec 11 17:19:23.342 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-8000/special_tokens_map.json\n",
            "Dec 11 17:19:23.733 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-7000] due to args.save_total_limit\n",
            "Dec 11 17:19:40.236 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3288 samples for epoch #17 from 2 peers. ETA 34.37 sec (refresh in 13.75 sec)\n",
            "Dec 11 17:19:54.102 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3592 samples for epoch #17 from 2 peers. ETA 23.04 sec (refresh in 9.21 sec)\n",
            "Dec 11 17:20:03.435 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3800 samples for epoch #17 from 2 peers. ETA 12.28 sec (refresh in 4.91 sec)\n",
            "Dec 11 17:20:08.465 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #17 from 2 peers. ETA 8.06 sec (refresh in 3.23 sec)\n",
            "Dec 11 17:20:11.732 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:20:11.807 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3976 samples for epoch #17 from 2 peers. ETA 4.93 sec (refresh in 1.97 sec)\n",
            "Dec 11 17:20:13.895 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #17 from 2 peers. ETA 2.71 sec (refresh in 1.08 sec)\n",
            "Dec 11 17:20:15.095 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4048 samples for epoch #17 from 2 peers. ETA 1.57 sec (refresh in 0.63 sec)\n",
            "Dec 11 17:20:15.837 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #17 from 2 peers. ETA 0.83 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:20:16.454 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #17 from 2 peers. ETA 0.13 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:20:17.071 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4096 samples for epoch #17 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:20:17.071 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #17\n",
            "Dec 11 17:20:17.099 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 18\n",
            "Dec 11 17:20:17.105 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #18\n",
            "Dec 11 17:20:17.106 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 32536 samples\n",
            "Dec 11 17:20:17.106 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.350 samples/sec\n",
            "Dec 11 17:20:17.106 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00623\n",
            "Dec 11 17:20:17.700 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #18 from 2 peers. ETA 176.11 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:20:23.119 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:20:30.723 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:20:31.152 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:20:47.816 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 656 samples for epoch #18 from 2 peers. ETA 155.45 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:21:17.932 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1360 samples for epoch #18 from 2 peers. ETA 114.73 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:21:48.049 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2036 samples for epoch #18 from 2 peers. ETA 95.86 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:22:17.222 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-8500\n",
            "Dec 11 17:22:17.224 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-8500/config.json\n",
            "Dec 11 17:22:17.367 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-8500/model.safetensors\n",
            "Dec 11 17:22:17.368 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-8500/tokenizer_config.json\n",
            "Dec 11 17:22:17.368 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-8500/special_tokens_map.json\n",
            "Dec 11 17:22:17.755 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-7500] due to args.save_total_limit\n",
            "Dec 11 17:22:18.166 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2724 samples for epoch #18 from 2 peers. ETA 64.58 sec (refresh in 25.83 sec)\n",
            "Dec 11 17:22:44.122 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3288 samples for epoch #18 from 2 peers. ETA 36.57 sec (refresh in 14.63 sec)\n",
            "Dec 11 17:22:58.868 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3644 samples for epoch #18 from 2 peers. ETA 19.06 sec (refresh in 7.62 sec)\n",
            "Dec 11 17:23:06.607 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3828 samples for epoch #18 from 2 peers. ETA 11.02 sec (refresh in 4.41 sec)\n",
            "Dec 11 17:23:11.135 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3932 samples for epoch #18 from 2 peers. ETA 6.44 sec (refresh in 2.57 sec)\n",
            "Dec 11 17:23:12.802 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:23:13.829 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3996 samples for epoch #18 from 2 peers. ETA 4.05 sec (refresh in 1.62 sec)\n",
            "Dec 11 17:23:15.568 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4028 samples for epoch #18 from 2 peers. ETA 2.40 sec (refresh in 0.96 sec)\n",
            "Dec 11 17:23:16.649 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4056 samples for epoch #18 from 2 peers. ETA 1.40 sec (refresh in 0.56 sec)\n",
            "Dec 11 17:23:17.324 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4072 samples for epoch #18 from 2 peers. ETA 0.87 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:23:17.945 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #18 from 2 peers. ETA 0.14 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:23:18.257 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #18\n",
            "Dec 11 17:23:18.282 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 19\n",
            "Dec 11 17:23:18.288 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #19\n",
            "Dec 11 17:23:18.288 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 34616 samples\n",
            "Dec 11 17:23:18.289 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 10.902 samples/sec\n",
            "Dec 11 17:23:18.289 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01492\n",
            "Dec 11 17:23:18.561 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #19 from 2 peers. ETA 186.68 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:23:26.568 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:23:35.264 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:23:35.436 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:23:48.675 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 688 samples for epoch #19 from 2 peers. ETA 145.84 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:24:18.793 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1368 samples for epoch #19 from 2 peers. ETA 112.42 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:24:48.906 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2056 samples for epoch #19 from 2 peers. ETA 85.75 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:25:10.305 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-9000\n",
            "Dec 11 17:25:10.306 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-9000/config.json\n",
            "Dec 11 17:25:10.453 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-9000/model.safetensors\n",
            "Dec 11 17:25:10.454 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-9000/tokenizer_config.json\n",
            "Dec 11 17:25:10.455 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-9000/special_tokens_map.json\n",
            "Dec 11 17:25:10.866 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-8000] due to args.save_total_limit\n",
            "Dec 11 17:25:19.027 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2740 samples for epoch #19 from 2 peers. ETA 60.74 sec (refresh in 24.29 sec)\n",
            "Dec 11 17:25:43.436 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3284 samples for epoch #19 from 2 peers. ETA 37.69 sec (refresh in 15.07 sec)\n",
            "Dec 11 17:25:58.628 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3616 samples for epoch #19 from 2 peers. ETA 23.03 sec (refresh in 9.21 sec)\n",
            "Dec 11 17:26:07.955 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3836 samples for epoch #19 from 2 peers. ETA 11.08 sec (refresh in 4.43 sec)\n",
            "Dec 11 17:26:12.504 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3944 samples for epoch #19 from 2 peers. ETA 6.14 sec (refresh in 2.45 sec)\n",
            "Dec 11 17:26:14.123 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:26:15.073 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4004 samples for epoch #19 from 2 peers. ETA 3.91 sec (refresh in 1.56 sec)\n",
            "Dec 11 17:26:16.749 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4040 samples for epoch #19 from 2 peers. ETA 2.14 sec (refresh in 0.86 sec)\n",
            "Dec 11 17:26:17.718 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4064 samples for epoch #19 from 2 peers. ETA 1.16 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:26:18.332 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4080 samples for epoch #19 from 2 peers. ETA 0.38 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:26:18.946 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4096 samples for epoch #19 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:26:18.947 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #19\n",
            "Dec 11 17:26:18.972 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 20\n",
            "Dec 11 17:26:18.977 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #20\n",
            "Dec 11 17:26:18.978 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 36688 samples\n",
            "Dec 11 17:26:18.978 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.686 samples/sec\n",
            "Dec 11 17:26:18.978 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 10.99955\n",
            "Dec 11 17:26:19.560 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #20 from 2 peers. ETA 170.53 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:26:27.691 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:26:36.330 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:26:36.348 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:26:49.703 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 672 samples for epoch #20 from 2 peers. ETA 139.06 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:27:19.825 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1352 samples for epoch #20 from 2 peers. ETA 122.70 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:27:49.950 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2032 samples for epoch #20 from 2 peers. ETA 90.62 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:28:04.932 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-9500\n",
            "Dec 11 17:28:04.934 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-9500/config.json\n",
            "Dec 11 17:28:05.074 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-9500/model.safetensors\n",
            "Dec 11 17:28:05.075 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-9500/tokenizer_config.json\n",
            "Dec 11 17:28:05.075 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-9500/special_tokens_map.json\n",
            "Dec 11 17:28:05.484 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-8500] due to args.save_total_limit\n",
            "Dec 11 17:28:20.067 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2708 samples for epoch #20 from 2 peers. ETA 62.89 sec (refresh in 25.16 sec)\n",
            "Dec 11 17:28:45.343 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3292 samples for epoch #20 from 2 peers. ETA 34.61 sec (refresh in 13.84 sec)\n",
            "Dec 11 17:28:59.303 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3596 samples for epoch #20 from 2 peers. ETA 22.40 sec (refresh in 8.96 sec)\n",
            "Dec 11 17:29:08.385 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3808 samples for epoch #20 from 2 peers. ETA 11.39 sec (refresh in 4.56 sec)\n",
            "Dec 11 17:29:13.058 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3908 samples for epoch #20 from 2 peers. ETA 7.76 sec (refresh in 3.10 sec)\n",
            "Dec 11 17:29:15.826 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:29:16.274 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3980 samples for epoch #20 from 2 peers. ETA 4.68 sec (refresh in 1.87 sec)\n",
            "Dec 11 17:29:18.259 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4024 samples for epoch #20 from 2 peers. ETA 2.47 sec (refresh in 0.99 sec)\n",
            "Dec 11 17:29:19.363 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #20 from 2 peers. ETA 1.54 sec (refresh in 0.62 sec)\n",
            "Dec 11 17:29:20.093 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #20 from 2 peers. ETA 0.85 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:29:20.709 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #20 from 2 peers. ETA 0.16 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:29:20.991 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #20\n",
            "Dec 11 17:29:21.017 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 21\n",
            "Dec 11 17:29:21.023 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #21\n",
            "Dec 11 17:29:21.024 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 38792 samples\n",
            "Dec 11 17:29:21.024 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.839 samples/sec\n",
            "Dec 11 17:29:21.024 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00141\n",
            "Dec 11 17:29:21.433 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #21 from 2 peers. ETA 177.03 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:29:29.106 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:29:37.737 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:29:37.770 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:29:51.548 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 680 samples for epoch #21 from 2 peers. ETA 135.57 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:30:21.664 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1356 samples for epoch #21 from 2 peers. ETA 117.90 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:30:51.779 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2040 samples for epoch #21 from 2 peers. ETA 90.34 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:30:58.056 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-10000\n",
            "Dec 11 17:30:58.058 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-10000/config.json\n",
            "Dec 11 17:30:58.198 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-10000/model.safetensors\n",
            "Dec 11 17:30:58.199 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-10000/tokenizer_config.json\n",
            "Dec 11 17:30:58.199 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-10000/special_tokens_map.json\n",
            "Dec 11 17:30:58.622 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-9000] due to args.save_total_limit\n",
            "Dec 11 17:31:21.894 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2704 samples for epoch #21 from 2 peers. ETA 60.80 sec (refresh in 24.32 sec)\n",
            "Dec 11 17:31:46.417 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3272 samples for epoch #21 from 2 peers. ETA 33.61 sec (refresh in 13.44 sec)\n",
            "Dec 11 17:31:59.976 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3592 samples for epoch #21 from 2 peers. ETA 21.05 sec (refresh in 8.42 sec)\n",
            "Dec 11 17:32:00.981 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T17:32:00+0000 lvl=info msg=\"received remote stop\" obj=tunnels.session\n",
            "Dec 11 17:32:00.982 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T17:32:00+0000 lvl=info msg=\"accept failed\" obj=tunnels.session obj=csess id=8a85d3a70348 err=\"reconnecting session closed\"\n",
            "Dec 11 17:32:00.982 [\u001b[1m\u001b[34mINFO\u001b[0m] t=2024-12-11T17:32:00+0000 lvl=info msg=\"failed to accept connection: Listener closed\" obj=tunnels.session clientid=9d042ddad14de894466db73024b351d9\n",
            "Dec 11 17:32:00.983 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpyngrok.process.ngrok._log_line:99\u001b[0m] t=2024-12-11T17:32:00+0000 lvl=warn msg=\"Stopping forwarder\" name=tcp-34021-e05aa995-82ae-4d1d-902f-bf08800498a9 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "Dec 11 17:32:08.510 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3788 samples for epoch #21 from 2 peers. ETA 12.97 sec (refresh in 5.19 sec)\n",
            "Dec 11 17:32:13.815 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3900 samples for epoch #21 from 2 peers. ETA 7.63 sec (refresh in 3.05 sec)\n",
            "Dec 11 17:32:16.566 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:32:16.981 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3980 samples for epoch #21 from 2 peers. ETA 4.47 sec (refresh in 1.79 sec)\n",
            "Dec 11 17:32:18.883 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4024 samples for epoch #21 from 2 peers. ETA 2.77 sec (refresh in 1.11 sec)\n",
            "Dec 11 17:32:20.162 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4064 samples for epoch #21 from 2 peers. ETA 0.93 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:32:20.777 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4080 samples for epoch #21 from 2 peers. ETA 0.31 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:32:21.393 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4088 samples for epoch #21 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:32:21.578 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #21\n",
            "Dec 11 17:32:21.604 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 22\n",
            "Dec 11 17:32:21.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #22\n",
            "Dec 11 17:32:21.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 40904 samples\n",
            "Dec 11 17:32:21.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 11.398 samples/sec\n",
            "Dec 11 17:32:21.610 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.01568\n",
            "Dec 11 17:32:22.008 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #22 from 2 peers. ETA 175.94 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:32:29.646 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:32:38.260 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:32:38.747 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:32:52.123 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 700 samples for epoch #22 from 2 peers. ETA 148.72 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:33:22.238 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1376 samples for epoch #22 from 2 peers. ETA 125.60 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:33:47.568 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-10500\n",
            "Dec 11 17:33:47.570 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-10500/config.json\n",
            "Dec 11 17:33:47.714 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-10500/model.safetensors\n",
            "Dec 11 17:33:47.715 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-10500/tokenizer_config.json\n",
            "Dec 11 17:33:47.716 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-10500/special_tokens_map.json\n",
            "Dec 11 17:33:48.150 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-9500] due to args.save_total_limit\n",
            "Dec 11 17:33:52.352 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2072 samples for epoch #22 from 2 peers. ETA 91.45 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:34:22.470 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2760 samples for epoch #22 from 2 peers. ETA 60.67 sec (refresh in 24.27 sec)\n",
            "Dec 11 17:34:46.853 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3300 samples for epoch #22 from 2 peers. ETA 32.86 sec (refresh in 13.14 sec)\n",
            "Dec 11 17:35:00.114 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3600 samples for epoch #22 from 2 peers. ETA 21.38 sec (refresh in 8.55 sec)\n",
            "Dec 11 17:35:08.787 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3792 samples for epoch #22 from 2 peers. ETA 12.78 sec (refresh in 5.11 sec)\n",
            "Dec 11 17:35:14.015 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3912 samples for epoch #22 from 2 peers. ETA 7.93 sec (refresh in 3.17 sec)\n",
            "Dec 11 17:35:17.191 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:35:17.304 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3984 samples for epoch #22 from 2 peers. ETA 4.14 sec (refresh in 1.66 sec)\n",
            "Dec 11 17:35:19.076 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4020 samples for epoch #22 from 2 peers. ETA 2.51 sec (refresh in 1.00 sec)\n",
            "Dec 11 17:35:20.197 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #22 from 2 peers. ETA 1.51 sec (refresh in 0.60 sec)\n",
            "Dec 11 17:35:20.938 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4068 samples for epoch #22 from 2 peers. ETA 0.77 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:35:21.554 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4084 samples for epoch #22 from 2 peers. ETA 0.10 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:35:21.821 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #22\n",
            "Dec 11 17:35:21.847 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 23\n",
            "Dec 11 17:35:21.852 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #23\n",
            "Dec 11 17:35:21.853 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 42960 samples\n",
            "Dec 11 17:35:21.853 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.228 samples/sec\n",
            "Dec 11 17:35:21.853 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00390\n",
            "Dec 11 17:35:22.168 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 0 samples for epoch #23 from 2 peers. ETA 172.88 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:35:30.213 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged gradients with 2 peers\n",
            "Dec 11 17:35:38.531 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 11 17:35:38.580 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 11 17:35:52.287 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 696 samples for epoch #23 from 2 peers. ETA 147.66 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:36:22.400 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 1364 samples for epoch #23 from 2 peers. ETA 120.96 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:36:43.075 [\u001b[1m\u001b[34mINFO\u001b[0m] Saving model checkpoint to outputs/checkpoint-11000\n",
            "Dec 11 17:36:43.078 [\u001b[1m\u001b[34mINFO\u001b[0m] Configuration saved in outputs/checkpoint-11000/config.json\n",
            "Dec 11 17:36:43.224 [\u001b[1m\u001b[34mINFO\u001b[0m] Model weights saved in outputs/checkpoint-11000/model.safetensors\n",
            "Dec 11 17:36:43.225 [\u001b[1m\u001b[34mINFO\u001b[0m] tokenizer config file saved in outputs/checkpoint-11000/tokenizer_config.json\n",
            "Dec 11 17:36:43.225 [\u001b[1m\u001b[34mINFO\u001b[0m] Special tokens file saved in outputs/checkpoint-11000/special_tokens_map.json\n",
            "Dec 11 17:36:43.674 [\u001b[1m\u001b[34mINFO\u001b[0m] Deleting older checkpoint [outputs/checkpoint-10000] due to args.save_total_limit\n",
            "Dec 11 17:36:52.512 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2048 samples for epoch #23 from 2 peers. ETA 82.35 sec (refresh in 30.00 sec)\n",
            "Dec 11 17:37:22.637 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 2704 samples for epoch #23 from 2 peers. ETA 60.91 sec (refresh in 24.36 sec)\n",
            "Dec 11 17:37:47.157 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3268 samples for epoch #23 from 2 peers. ETA 38.04 sec (refresh in 15.21 sec)\n",
            "Dec 11 17:38:02.492 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3628 samples for epoch #23 from 2 peers. ETA 20.23 sec (refresh in 8.09 sec)\n",
            "Dec 11 17:38:10.698 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3816 samples for epoch #23 from 2 peers. ETA 12.02 sec (refresh in 4.81 sec)\n",
            "Dec 11 17:38:15.620 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 3932 samples for epoch #23 from 2 peers. ETA 6.66 sec (refresh in 2.66 sec)\n",
            "Dec 11 17:38:17.641 [\u001b[1m\u001b[34mINFO\u001b[0m] Pre-scheduling gradient averaging round in 5.00 sec\n",
            "Dec 11 17:38:18.395 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4000 samples for epoch #23 from 2 peers. ETA 3.74 sec (refresh in 1.50 sec)\n",
            "Dec 11 17:38:20.003 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4032 samples for epoch #23 from 2 peers. ETA 2.53 sec (refresh in 1.01 sec)\n",
            "Dec 11 17:38:21.127 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4052 samples for epoch #23 from 2 peers. ETA 1.39 sec (refresh in 0.55 sec)\n",
            "Dec 11 17:38:21.851 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4064 samples for epoch #23 from 2 peers. ETA 0.91 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:38:22.554 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4076 samples for epoch #23 from 2 peers. ETA 0.42 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:38:23.177 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4100 samples for epoch #23 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 11 17:38:23.217 [\u001b[1m\u001b[34mINFO\u001b[0m] Beginning optimizer step #23\n",
            "Dec 11 17:38:23.242 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 24\n",
            "Dec 11 17:38:23.248 [\u001b[1m\u001b[34mINFO\u001b[0m] Step #24\n",
            "Dec 11 17:38:23.248 [\u001b[1m\u001b[34mINFO\u001b[0m] Your current contribution: 45076 samples\n",
            "Dec 11 17:38:23.248 [\u001b[1m\u001b[34mINFO\u001b[0m] Performance: 12.008 samples/sec\n",
            "Dec 11 17:38:23.248 [\u001b[1m\u001b[34mINFO\u001b[0m] Local loss: 11.00431\n",
            "Dec 11 17:38:23.857 [\u001b[1m\u001b[34mINFO\u001b[0m] albert accumulated 4 samples for epoch #24 from 2 peers. ETA 178.94 sec (refresh in 30.00 sec)\n"
          ]
        }
      ],
      "source": [
        "!./run_trainer.py  --initial_peers /dns4/0.tcp.jp.ngrok.io/tcp/12810/p2p/12D3KooWCkxGoZdmfXaGqhvHKQVCrcBtF6SEjbqDkbgHJHrTwbd2 --per_device_train_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc9hFm0rlX_m",
        "outputId": "b49b998c-fa36-4d51-a3cf-560f90ce6395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 13:37:55.538017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 13:37:55.563548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 13:37:55.570958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 13:37:55.590110: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 13:37:56.842755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Dec 11 13:37:58.543 [\u001b[1m\u001b[34mINFO\u001b[0m] Found 1 initial peers: ['/dns4/2.tcp.eu.ngrok.io/tcp/18041/p2p/12D3KooWDRHR7dG2G1nZZgjbNCr9MjHpWiD2gau7g6VSrveNVHCJ']\n",
            "Dec 11 13:37:58.544 [\u001b[1m\u001b[34mINFO\u001b[0m] Training/evaluation parameters:\n",
            "AlbertTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "clamp_value=10000.0,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=2,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O2,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.00176,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=1000000000000000000000000000000,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "seq_length=512,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "total_steps=125000,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=5000,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "config.json: 100% 685/685 [00:00<00:00, 4.58MB/s]\n",
            "Dec 11 13:37:58.994 [\u001b[1m\u001b[34mINFO\u001b[0m] loading configuration file config.json from cache at data/models--albert-large-v2/snapshots/dfed3a5ef4499fb3351c4ebbcf487375d1e942c8/config.json\n",
            "Dec 11 13:37:58.996 [\u001b[1m\u001b[34mINFO\u001b[0m] Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Dec 11 13:37:58.997 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file spiece.model\n",
            "Dec 11 13:37:58.997 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer.json\n",
            "Dec 11 13:37:58.997 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file added_tokens.json\n",
            "Dec 11 13:37:58.997 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file special_tokens_map.json\n",
            "Dec 11 13:37:58.997 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer_config.json\n",
            "Dec 11 13:37:59.079 [\u001b[1m\u001b[34mINFO\u001b[0m] Checkpoint dir outputs, contents []\n",
            "Dec 11 13:37:59.079 [\u001b[1m\u001b[34mINFO\u001b[0m] Training from scratch\n",
            "Dec 11 13:37:59.446 [\u001b[1m\u001b[34mINFO\u001b[0m] You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/./run_trainer.py\", line 325, in <module>\n",
            "    main()\n",
            "  File \"/content/./run_trainer.py\", line 220, in main\n",
            "    dht = DHT(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 88, in __init__\n",
            "    self.run_in_background(await_ready=await_ready)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 149, in run_in_background\n",
            "    self.wait_until_ready(timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 152, in wait_until_ready\n",
            "    self._ready.result(timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/utils/mpfuture.py\", line 254, in result\n",
            "    return super().result(timeout)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "hivemind.p2p.p2p_daemon_bindings.utils.P2PDaemonError: Daemon failed to start: 2024/12/11 13:38:04 failed to connect to bootstrap peers\n"
          ]
        }
      ],
      "source": [
        "!./run_trainer.py  --initial_peers /dns4/2.tcp.eu.ngrok.io/tcp/18041/p2p/12D3KooWDRHR7dG2G1nZZgjbNCr9MjHpWiD2gau7g6VSrveNVHCJ --per_device_train_batch_size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqTe85nYcP0q",
        "outputId": "80596642-72f5-4676-f2d2-e035c6af1894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-12-11 07:18:02.847439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 07:18:02.866544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 07:18:02.872374: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 07:18:02.886000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 07:18:03.942432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Dec 11 07:18:05.451 [\u001b[1m\u001b[34mINFO\u001b[0m] Found 1 initial peers: ['/ip4/52.77.3.235/tcp/15936/p2p/12D3KooWRRxmVHQWyDCSMuDUDKarXAEZcr7EmrLmXisV2yPW97N5']\n",
            "Dec 11 07:18:05.452 [\u001b[1m\u001b[34mINFO\u001b[0m] Training/evaluation parameters:\n",
            "AlbertTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "clamp_value=10000.0,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=2,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O2,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.00176,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=1000000000000000000000000000000,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "seq_length=512,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "total_steps=125000,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=5000,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "Dec 11 07:18:05.748 [\u001b[1m\u001b[34mINFO\u001b[0m] loading configuration file config.json from cache at data/models--albert-large-v2/snapshots/dfed3a5ef4499fb3351c4ebbcf487375d1e942c8/config.json\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file spiece.model\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer.json\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file added_tokens.json\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file special_tokens_map.json\n",
            "Dec 11 07:18:05.749 [\u001b[1m\u001b[34mINFO\u001b[0m] loading file tokenizer_config.json\n",
            "Dec 11 07:18:05.801 [\u001b[1m\u001b[34mINFO\u001b[0m] Checkpoint dir outputs, contents []\n",
            "Dec 11 07:18:05.801 [\u001b[1m\u001b[34mINFO\u001b[0m] Training from scratch\n",
            "Dec 11 07:18:06.102 [\u001b[1m\u001b[34mINFO\u001b[0m] You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/./run_trainer.py\", line 325, in <module>\n",
            "    main()\n",
            "  File \"/content/./run_trainer.py\", line 220, in main\n",
            "    dht = DHT(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 88, in __init__\n",
            "    self.run_in_background(await_ready=await_ready)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 149, in run_in_background\n",
            "    self.wait_until_ready(timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/dht/dht.py\", line 152, in wait_until_ready\n",
            "    self._ready.result(timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/utils/mpfuture.py\", line 254, in result\n",
            "    return super().result(timeout)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "hivemind.p2p.p2p_daemon_bindings.utils.P2PDaemonError: Daemon failed to start: 2024/12/11 07:18:10 failed to connect to bootstrap peers\n"
          ]
        }
      ],
      "source": [
        "!./run_trainer.py  --initial_peers /ip4/{ip_address}/tcp/15936/p2p/12D3KooWR2Pws5rEvtR167RnYYbCUMV51ErShmTFm55si36YrntS --per_device_train_batch_size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjzqg3NCcYAY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "wnMcP-3USHCz",
        "outputId": "ab062ca8-cf41-4f0a-8670-949d3af7b6a1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'hivemind' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a0f62a14e400>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import hivemind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dht = hivemind.DHT(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mhost_maddrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/ip4/0.0.0.0/tcp/0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/ip4/0.0.0.0/udp/0/quic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     start=True)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hivemind' is not defined"
          ]
        }
      ],
      "source": [
        "import hivemind\n",
        "dht = hivemind.DHT(\n",
        "    host_maddrs=[\"/ip4/0.0.0.0/tcp/0\", \"/ip4/0.0.0.0/udp/0/quic\"],\n",
        "    start=True)\n",
        "\n",
        "print('\\n'.join(str(addr) for addr in dht.get_visible_maddrs()))\n",
        "print(\"Global IP:\", hivemind.utils.networking.choose_ip_address(dht.get_visible_maddrs()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rspjs0HL4Hop",
        "outputId": "8fb7e937-e811-4f83-beb2-30ad72e21945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IP Address of 0.tcp.jp.ngrok.io: 35.76.159.227\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "\n",
        "# 输入你的 ngrok 公网地址\n",
        "ngrok_host = \"0.tcp.jp.ngrok.io\"\n",
        "\n",
        "# 获取 IP 地址\n",
        "ip_address = socket.gethostbyname(ngrok_host)\n",
        "\n",
        "# 输出 IP 地址\n",
        "print(f\"IP Address of {ngrok_host}: {ip_address}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d4208eb89de342aba6b90408c476312f",
            "943cc0e49e1e4211b718440252381750",
            "0f3a0a362a714d8bbb6f96eac7c9e2a2",
            "18f0df7e5e844cf48109442f79851652",
            "91a0adb1c7234f458ba0ee2b0d96ea38",
            "a21052c85ece4b4fa65a6f0ce72c99ae",
            "e65a65f7bcbc42828ae6601a7c2addd8",
            "8eb16492fdbe468cb5e4833997176ef4",
            "05e661dcbd6f44bfaa7f22c1354e6810",
            "dc194b33400f4e65a64a071e59e9f430",
            "756252a23c474f0981707478ea6d9ce3",
            "abfa51b4884042b8ab465752d3e4a7df",
            "a92722172922468b93e0065e8992a549",
            "080d26b376f243f6b33655b978f7d822",
            "0ea3abd6bb15432384c71d97f5819508",
            "02e3ae6e1e28406d90a8867e66424e64",
            "c149fb64aee540518facfc37823db46b",
            "4b147433804f429aafe1416e5e1056fd",
            "9e682648ddb544baa373415eb141a9eb",
            "1da0d0041ea84b989734b40d38cbda0c",
            "99e01f47b4744683a39650e920209823",
            "adef8f315fe545ac9b8f7322cbaf76e4"
          ]
        },
        "id": "XkhGVgjLUBiV",
        "outputId": "ccc15b23-8213-44c2-ca85-c086e0a95b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4208eb89de342aba6b90408c476312f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dec 10 09:04:56.311 [\u001b[1m\u001b[34mINFO\u001b[0m] Found no active peers: None\n",
            "Dec 10 09:04:56.321 [\u001b[1m\u001b[34mINFO\u001b[0m] Initializing optimizer manually since it has no tensors in state dict. To override this, provide initialize_optimizer=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To join the training, use initial_peers = ['/ip4/172.28.0.12/tcp/33539/p2p/12D3KooWE851VaSCr8NTw1ZkSKdpp1Xu5Ke6rxf3CmwzNwu7z3WW', '/ip4/127.0.0.1/tcp/33539/p2p/12D3KooWE851VaSCr8NTw1ZkSKdpp1Xu5Ke6rxf3CmwzNwu7z3WW', '/ip4/172.28.0.12/udp/42771/quic/p2p/12D3KooWE851VaSCr8NTw1ZkSKdpp1Xu5Ke6rxf3CmwzNwu7z3WW', '/ip4/127.0.0.1/udp/42771/quic/p2p/12D3KooWE851VaSCr8NTw1ZkSKdpp1Xu5Ke6rxf3CmwzNwu7z3WW']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dec 10 09:04:56.391 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mhivemind.averaging.matchmaking.__init__:54\u001b[0m] It is recommended to use request_timeout smaller than min_matchmaking_time. Otherwise, matchmaking can cause deadlocks in some rare cases. Please see Matchmaking docstring.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abfa51b4884042b8ab465752d3e4a7df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dec 10 09:04:59.331 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2016 samples for epoch #0 from 1 peers. ETA 10.98 sec (refresh in 2.75 sec)\n",
            "Dec 10 09:05:02.081 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4224 samples for epoch #0 from 1 peers. ETA 5.42 sec (refresh in 1.35 sec)\n",
            "Dec 10 09:05:03.441 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5664 samples for epoch #0 from 1 peers. ETA 3.95 sec (refresh in 0.99 sec)\n",
            "Dec 10 09:05:04.435 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6720 samples for epoch #0 from 1 peers. ETA 2.98 sec (refresh in 0.74 sec)\n",
            "Dec 10 09:05:05.193 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7520 samples for epoch #0 from 1 peers. ETA 2.26 sec (refresh in 0.57 sec)\n",
            "Dec 10 09:05:05.767 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8064 samples for epoch #0 from 1 peers. ETA 1.85 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:06.273 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8608 samples for epoch #0 from 1 peers. ETA 1.32 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:06.784 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9120 samples for epoch #0 from 1 peers. ETA 0.85 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:07.294 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9664 samples for epoch #0 from 1 peers. ETA 0.31 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:07.639 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 1\n",
            "Dec 10 09:05:07.799 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #1 from 1 peers. ETA 9.31 sec (refresh in 2.33 sec)\n",
            "Dec 10 09:05:10.142 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2624 samples for epoch #1 from 1 peers. ETA 6.90 sec (refresh in 1.72 sec)\n",
            "Dec 10 09:05:11.870 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4224 samples for epoch #1 from 1 peers. ETA 7.48 sec (refresh in 1.87 sec)\n",
            "Dec 10 09:05:13.746 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5664 samples for epoch #1 from 1 peers. ETA 5.78 sec (refresh in 1.44 sec)\n",
            "Dec 10 09:05:15.195 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6688 samples for epoch #1 from 1 peers. ETA 4.66 sec (refresh in 1.17 sec)\n",
            "Dec 10 09:05:16.367 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7712 samples for epoch #1 from 1 peers. ETA 2.27 sec (refresh in 0.57 sec)\n",
            "Dec 10 09:05:16.941 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8288 samples for epoch #1 from 1 peers. ETA 1.61 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:17.450 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8832 samples for epoch #1 from 1 peers. ETA 1.05 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:17.957 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9376 samples for epoch #1 from 1 peers. ETA 0.57 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:18.470 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9888 samples for epoch #1 from 1 peers. ETA 0.08 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:18.568 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 2\n",
            "Dec 10 09:05:18.975 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 416 samples for epoch #2 from 1 peers. ETA 8.85 sec (refresh in 2.21 sec)\n",
            "Dec 10 09:05:21.196 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2752 samples for epoch #2 from 1 peers. ETA 6.89 sec (refresh in 1.72 sec)\n",
            "Dec 10 09:05:22.927 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4544 samples for epoch #2 from 1 peers. ETA 5.03 sec (refresh in 1.26 sec)\n",
            "Dec 10 09:05:24.195 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5888 samples for epoch #2 from 1 peers. ETA 3.80 sec (refresh in 0.95 sec)\n",
            "Dec 10 09:05:25.160 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6912 samples for epoch #2 from 1 peers. ETA 2.82 sec (refresh in 0.71 sec)\n",
            "Dec 10 09:05:25.891 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7648 samples for epoch #2 from 1 peers. ETA 2.32 sec (refresh in 0.58 sec)\n",
            "Dec 10 09:05:26.477 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8064 samples for epoch #2 from 1 peers. ETA 2.48 sec (refresh in 0.62 sec)\n",
            "Dec 10 09:05:27.105 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #2 from 1 peers. ETA 1.91 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:27.609 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8896 samples for epoch #2 from 1 peers. ETA 1.47 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:28.121 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9280 samples for epoch #2 from 1 peers. ETA 0.92 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:28.625 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9632 samples for epoch #2 from 1 peers. ETA 0.48 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:29.112 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 3\n",
            "Dec 10 09:05:29.140 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #3 from 1 peers. ETA 13.57 sec (refresh in 3.39 sec)\n",
            "Dec 10 09:05:32.538 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3040 samples for epoch #3 from 1 peers. ETA 6.66 sec (refresh in 1.66 sec)\n",
            "Dec 10 09:05:34.210 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4800 samples for epoch #3 from 1 peers. ETA 4.88 sec (refresh in 1.22 sec)\n",
            "Dec 10 09:05:35.434 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6080 samples for epoch #3 from 1 peers. ETA 3.62 sec (refresh in 0.91 sec)\n",
            "Dec 10 09:05:36.346 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7008 samples for epoch #3 from 1 peers. ETA 2.85 sec (refresh in 0.71 sec)\n",
            "Dec 10 09:05:37.064 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7744 samples for epoch #3 from 1 peers. ETA 2.18 sec (refresh in 0.54 sec)\n",
            "Dec 10 09:05:37.612 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8320 samples for epoch #3 from 1 peers. ETA 1.65 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:38.117 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8864 samples for epoch #3 from 1 peers. ETA 1.03 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:38.625 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9376 samples for epoch #3 from 1 peers. ETA 0.60 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:39.139 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9920 samples for epoch #3 from 1 peers. ETA 0.05 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:39.209 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 4\n",
            "Dec 10 09:05:39.645 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 416 samples for epoch #4 from 1 peers. ETA 9.79 sec (refresh in 2.45 sec)\n",
            "Dec 10 09:05:42.105 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2624 samples for epoch #4 from 1 peers. ETA 9.86 sec (refresh in 2.47 sec)\n",
            "Dec 10 09:05:44.583 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4384 samples for epoch #4 from 1 peers. ETA 8.23 sec (refresh in 2.06 sec)\n",
            "Dec 10 09:05:46.647 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6304 samples for epoch #4 from 1 peers. ETA 3.58 sec (refresh in 0.90 sec)\n",
            "Dec 10 09:05:47.548 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7232 samples for epoch #4 from 1 peers. ETA 2.65 sec (refresh in 0.66 sec)\n",
            "Dec 10 09:05:48.215 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7968 samples for epoch #4 from 1 peers. ETA 1.86 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:48.721 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8480 samples for epoch #4 from 1 peers. ETA 1.45 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:49.225 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9024 samples for epoch #4 from 1 peers. ETA 0.87 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:49.732 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9536 samples for epoch #4 from 1 peers. ETA 0.43 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:05:50.155 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 5\n",
            "Dec 10 09:05:50.237 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #5 from 1 peers. ETA 9.06 sec (refresh in 2.27 sec)\n",
            "Dec 10 09:05:52.510 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2432 samples for epoch #5 from 1 peers. ETA 7.60 sec (refresh in 1.90 sec)\n",
            "Dec 10 09:05:54.414 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4448 samples for epoch #5 from 1 peers. ETA 5.07 sec (refresh in 1.27 sec)\n",
            "Dec 10 09:05:55.693 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5664 samples for epoch #5 from 1 peers. ETA 5.10 sec (refresh in 1.27 sec)\n",
            "Dec 10 09:05:56.982 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6624 samples for epoch #5 from 1 peers. ETA 4.28 sec (refresh in 1.07 sec)\n",
            "Dec 10 09:05:58.062 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7424 samples for epoch #5 from 1 peers. ETA 3.37 sec (refresh in 0.84 sec)\n",
            "Dec 10 09:05:58.919 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8032 samples for epoch #5 from 1 peers. ETA 2.81 sec (refresh in 0.70 sec)\n",
            "Dec 10 09:05:59.637 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #5 from 1 peers. ETA 2.18 sec (refresh in 0.54 sec)\n",
            "Dec 10 09:06:00.187 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8992 samples for epoch #5 from 1 peers. ETA 1.13 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:00.694 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9504 samples for epoch #5 from 1 peers. ETA 0.51 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:01.165 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 6\n",
            "Dec 10 09:06:01.199 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #6 from 1 peers. ETA 9.46 sec (refresh in 2.37 sec)\n",
            "Dec 10 09:06:03.595 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #6 from 1 peers. ETA 7.14 sec (refresh in 1.79 sec)\n",
            "Dec 10 09:06:05.390 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4448 samples for epoch #6 from 1 peers. ETA 5.11 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:06:06.685 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5728 samples for epoch #6 from 1 peers. ETA 4.30 sec (refresh in 1.08 sec)\n",
            "Dec 10 09:06:07.771 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6880 samples for epoch #6 from 1 peers. ETA 2.90 sec (refresh in 0.72 sec)\n",
            "Dec 10 09:06:08.500 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7648 samples for epoch #6 from 1 peers. ETA 2.19 sec (refresh in 0.55 sec)\n",
            "Dec 10 09:06:09.056 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8256 samples for epoch #6 from 1 peers. ETA 1.57 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:09.560 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8736 samples for epoch #6 from 1 peers. ETA 1.23 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:10.068 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9248 samples for epoch #6 from 1 peers. ETA 0.71 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:10.574 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9600 samples for epoch #6 from 1 peers. ETA 0.49 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:11.080 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9984 samples for epoch #6 from 1 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:11.105 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 7\n",
            "Dec 10 09:06:11.585 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 320 samples for epoch #7 from 1 peers. ETA 13.24 sec (refresh in 3.31 sec)\n",
            "Dec 10 09:06:15.106 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2880 samples for epoch #7 from 1 peers. ETA 7.32 sec (refresh in 1.83 sec)\n",
            "Dec 10 09:06:17.147 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5312 samples for epoch #7 from 2 peers. ETA 2.51 sec (refresh in 1.00 sec)\n",
            "Dec 10 09:06:18.360 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7168 samples for epoch #7 from 2 peers. ETA 1.50 sec (refresh in 0.60 sec)\n",
            "Dec 10 09:06:19.160 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #7 from 2 peers. ETA 0.59 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:19.872 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9536 samples for epoch #7 from 2 peers. ETA 0.01 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:19.874 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 8\n",
            "Dec 10 09:06:19.919 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:06:20.566 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #8 from 2 peers. ETA 6.04 sec (refresh in 2.42 sec)\n",
            "Dec 10 09:06:23.176 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #8 from 2 peers. ETA 3.43 sec (refresh in 1.37 sec)\n",
            "Dec 10 09:06:23.567 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:06:23.576 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:06:24.768 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1504 samples for epoch #8 from 2 peers. ETA 9.99 sec (refresh in 4.00 sec)\n",
            "Dec 10 09:06:29.001 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6144 samples for epoch #8 from 2 peers. ETA 3.21 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:06:30.485 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8096 samples for epoch #8 from 2 peers. ETA 1.04 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:31.183 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9024 samples for epoch #8 from 2 peers. ETA 0.46 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:31.641 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 9\n",
            "Dec 10 09:06:31.744 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:06:31.929 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #9 from 2 peers. ETA 6.38 sec (refresh in 2.55 sec)\n",
            "Dec 10 09:06:34.679 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #9 from 2 peers. ETA 4.17 sec (refresh in 1.67 sec)\n",
            "Dec 10 09:06:35.254 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:06:35.259 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:06:36.557 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1856 samples for epoch #9 from 2 peers. ETA 7.32 sec (refresh in 2.93 sec)\n",
            "Dec 10 09:06:39.703 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6752 samples for epoch #9 from 2 peers. ETA 1.76 sec (refresh in 0.71 sec)\n",
            "Dec 10 09:06:40.602 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7840 samples for epoch #9 from 2 peers. ETA 1.58 sec (refresh in 0.63 sec)\n",
            "Dec 10 09:06:41.453 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8768 samples for epoch #9 from 2 peers. ETA 0.88 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:42.149 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9600 samples for epoch #9 from 2 peers. ETA 0.08 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:42.255 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 10\n",
            "Dec 10 09:06:42.307 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:06:42.844 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #10 from 2 peers. ETA 8.77 sec (refresh in 3.51 sec)\n",
            "Dec 10 09:06:45.835 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:06:45.838 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:06:46.553 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 800 samples for epoch #10 from 2 peers. ETA 21.34 sec (refresh in 8.54 sec)\n",
            "Dec 10 09:06:55.305 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 13696 samples for epoch #10 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:06:55.357 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 11\n",
            "Dec 10 09:06:55.429 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:06:55.999 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #11 from 2 peers. ETA 8.54 sec (refresh in 3.42 sec)\n",
            "Dec 10 09:06:58.981 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:06:58.987 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:06:59.625 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 640 samples for epoch #11 from 2 peers. ETA 23.71 sec (refresh in 9.48 sec)\n",
            "Dec 10 09:07:09.322 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #12 from 2 peers. ETA 5.15 sec (refresh in 2.06 sec)\n",
            "Dec 10 09:07:09.360 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 12\n",
            "Dec 10 09:07:09.433 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:07:11.574 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #12 from 2 peers. ETA 3.06 sec (refresh in 1.22 sec)\n",
            "Dec 10 09:07:12.024 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:07:12.027 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:07:13.003 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1344 samples for epoch #12 from 2 peers. ETA 11.68 sec (refresh in 4.67 sec)\n",
            "Dec 10 09:07:17.872 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9152 samples for epoch #12 from 2 peers. ETA 0.31 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:07:18.199 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 13\n",
            "Dec 10 09:07:18.243 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:07:18.564 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #13 from 2 peers. ETA 5.86 sec (refresh in 2.35 sec)\n",
            "Dec 10 09:07:21.110 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #13 from 2 peers. ETA 4.75 sec (refresh in 1.90 sec)\n",
            "Dec 10 09:07:23.203 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #13 from 2 peers. ETA 3.54 sec (refresh in 1.42 sec)\n",
            "Dec 10 09:07:24.792 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:07:24.795 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:07:24.817 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #13 from 2 peers. ETA 1.99 sec (refresh in 0.80 sec)\n",
            "Dec 10 09:07:25.817 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1216 samples for epoch #13 from 2 peers. ETA 13.52 sec (refresh in 5.41 sec)\n",
            "Dec 10 09:07:31.427 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 10080 samples for epoch #13 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:07:31.440 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 14\n",
            "Dec 10 09:07:31.500 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:07:32.129 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #14 from 2 peers. ETA 5.68 sec (refresh in 2.27 sec)\n",
            "Dec 10 09:07:34.716 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #14 from 2 peers. ETA 3.63 sec (refresh in 1.45 sec)\n",
            "Dec 10 09:07:35.141 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:07:35.143 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:07:36.371 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1088 samples for epoch #14 from 2 peers. ETA 16.24 sec (refresh in 6.50 sec)\n",
            "Dec 10 09:07:43.065 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #15 from 2 peers. ETA 6.34 sec (refresh in 2.54 sec)\n",
            "Dec 10 09:07:43.096 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 15\n",
            "Dec 10 09:07:43.146 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:07:45.793 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #15 from 2 peers. ETA 3.50 sec (refresh in 1.40 sec)\n",
            "Dec 10 09:07:46.097 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:07:46.100 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:07:47.410 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1888 samples for epoch #15 from 2 peers. ETA 6.87 sec (refresh in 2.75 sec)\n",
            "Dec 10 09:07:50.356 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5760 samples for epoch #15 from 2 peers. ETA 3.31 sec (refresh in 1.33 sec)\n",
            "Dec 10 09:07:51.907 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7488 samples for epoch #15 from 2 peers. ETA 1.92 sec (refresh in 0.77 sec)\n",
            "Dec 10 09:07:52.878 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8576 samples for epoch #15 from 2 peers. ETA 1.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:07:53.593 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9504 samples for epoch #15 from 2 peers. ETA 0.09 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:07:53.703 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 16\n",
            "Dec 10 09:07:53.753 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:07:54.289 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #16 from 2 peers. ETA 6.78 sec (refresh in 2.71 sec)\n",
            "Dec 10 09:07:57.197 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #16 from 2 peers. ETA 3.87 sec (refresh in 1.55 sec)\n",
            "Dec 10 09:07:57.392 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:07:57.395 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:07:58.963 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2208 samples for epoch #16 from 2 peers. ETA 6.06 sec (refresh in 2.42 sec)\n",
            "Dec 10 09:08:01.591 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6464 samples for epoch #16 from 2 peers. ETA 1.96 sec (refresh in 0.79 sec)\n",
            "Dec 10 09:08:02.825 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8256 samples for epoch #16 from 2 peers. ETA 0.69 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:03.544 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9472 samples for epoch #16 from 2 peers. ETA 0.11 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:03.703 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 17\n",
            "Dec 10 09:08:03.752 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:08:04.238 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #17 from 2 peers. ETA 6.61 sec (refresh in 2.64 sec)\n",
            "Dec 10 09:08:07.074 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #17 from 2 peers. ETA 3.77 sec (refresh in 1.51 sec)\n",
            "Dec 10 09:08:07.338 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:08:07.341 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:08:08.808 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2048 samples for epoch #17 from 2 peers. ETA 6.66 sec (refresh in 2.66 sec)\n",
            "Dec 10 09:08:11.683 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5632 samples for epoch #17 from 2 peers. ETA 1.51 sec (refresh in 0.60 sec)\n",
            "Dec 10 09:08:12.498 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6432 samples for epoch #17 from 2 peers. ETA 0.73 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:13.197 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7072 samples for epoch #17 from 2 peers. ETA 0.05 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:13.254 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 18\n",
            "Dec 10 09:08:13.296 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:08:13.889 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 5.68 sec (refresh in 2.27 sec)\n",
            "Dec 10 09:08:16.355 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 4.29 sec (refresh in 1.72 sec)\n",
            "Dec 10 09:08:18.302 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 3.20 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:08:19.775 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 2.37 sec (refresh in 0.95 sec)\n",
            "Dec 10 09:08:20.917 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 1.73 sec (refresh in 0.69 sec)\n",
            "Dec 10 09:08:21.803 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 1.23 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:22.496 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 0.84 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:23.191 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #18 from 2 peers. ETA 0.45 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:23.265 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:08:23.268 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:08:23.637 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:08:23.680 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 19\n",
            "Dec 10 09:08:23.731 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:08:23.905 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #19 from 2 peers. ETA 12.04 sec (refresh in 4.82 sec)\n",
            "Dec 10 09:08:28.919 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #19 from 2 peers. ETA 11.20 sec (refresh in 4.48 sec)\n",
            "Dec 10 09:08:33.598 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #19 from 2 peers. ETA 10.58 sec (refresh in 4.23 sec)\n",
            "Dec 10 09:08:33.694 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:08:33.698 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:08:34.069 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:08:38.024 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3968 samples for epoch #19 from 2 peers. ETA 3.59 sec (refresh in 1.44 sec)\n",
            "Dec 10 09:08:39.775 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5664 samples for epoch #19 from 2 peers. ETA 2.48 sec (refresh in 0.99 sec)\n",
            "Dec 10 09:08:40.963 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6848 samples for epoch #19 from 2 peers. ETA 1.76 sec (refresh in 0.70 sec)\n",
            "Dec 10 09:08:41.875 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7552 samples for epoch #19 from 2 peers. ETA 1.55 sec (refresh in 0.62 sec)\n",
            "Dec 10 09:08:42.692 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8064 samples for epoch #19 from 2 peers. ETA 1.34 sec (refresh in 0.54 sec)\n",
            "Dec 10 09:08:43.425 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #19 from 2 peers. ETA 0.95 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:44.141 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9024 samples for epoch #19 from 2 peers. ETA 0.59 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:44.838 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9472 samples for epoch #19 from 2 peers. ETA 0.29 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:08:44.850 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 20\n",
            "Dec 10 09:08:44.908 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:08:45.570 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #20 from 2 peers. ETA 7.20 sec (refresh in 2.88 sec)\n",
            "Dec 10 09:08:48.644 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #20 from 2 peers. ETA 5.81 sec (refresh in 2.32 sec)\n",
            "Dec 10 09:08:51.163 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #20 from 2 peers. ETA 4.67 sec (refresh in 1.87 sec)\n",
            "Dec 10 09:08:53.223 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #20 from 2 peers. ETA 3.73 sec (refresh in 1.49 sec)\n",
            "Dec 10 09:08:54.859 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:08:54.862 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:08:54.912 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #20 from 2 peers. ETA 2.97 sec (refresh in 1.19 sec)\n",
            "Dec 10 09:08:55.228 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:08:56.308 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1024 samples for epoch #20 from 2 peers. ETA 8.15 sec (refresh in 3.26 sec)\n",
            "Dec 10 09:08:59.773 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3232 samples for epoch #20 from 2 peers. ETA 4.92 sec (refresh in 1.97 sec)\n",
            "Dec 10 09:09:01.953 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4928 samples for epoch #20 from 2 peers. ETA 2.98 sec (refresh in 1.19 sec)\n",
            "Dec 10 09:09:03.379 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6336 samples for epoch #20 from 2 peers. ETA 1.94 sec (refresh in 0.77 sec)\n",
            "Dec 10 09:09:04.355 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7296 samples for epoch #20 from 2 peers. ETA 1.43 sec (refresh in 0.57 sec)\n",
            "Dec 10 09:09:05.131 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8000 samples for epoch #20 from 2 peers. ETA 1.06 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:05.831 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8672 samples for epoch #20 from 2 peers. ETA 0.69 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:06.529 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9344 samples for epoch #20 from 2 peers. ETA 0.26 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:06.813 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 21\n",
            "Dec 10 09:09:06.858 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:09:07.227 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 5.86 sec (refresh in 2.34 sec)\n",
            "Dec 10 09:09:09.767 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 4.44 sec (refresh in 1.78 sec)\n",
            "Dec 10 09:09:11.739 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 3.34 sec (refresh in 1.34 sec)\n",
            "Dec 10 09:09:13.271 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 2.48 sec (refresh in 0.99 sec)\n",
            "Dec 10 09:09:14.458 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 1.82 sec (refresh in 0.73 sec)\n",
            "Dec 10 09:09:15.380 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 1.31 sec (refresh in 0.52 sec)\n",
            "Dec 10 09:09:16.095 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 0.91 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:16.787 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #21 from 2 peers. ETA 0.52 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:16.828 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:09:16.832 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:09:17.206 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:09:17.486 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 480 samples for epoch #21 from 2 peers. ETA 11.33 sec (refresh in 4.53 sec)\n",
            "Dec 10 09:09:17.490 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 22\n",
            "Dec 10 09:09:17.548 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:09:22.215 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #22 from 1 peers. ETA 63.62 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:09:27.513 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:09:27.516 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:09:27.885 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:09:32.438 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8768 samples for epoch #22 from 2 peers. ETA 0.64 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:33.145 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9696 samples for epoch #22 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:33.147 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 23\n",
            "Dec 10 09:09:33.209 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:09:33.845 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #23 from 2 peers. ETA 7.43 sec (refresh in 2.97 sec)\n",
            "Dec 10 09:09:37.019 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1408 samples for epoch #23 from 2 peers. ETA 3.59 sec (refresh in 1.43 sec)\n",
            "Dec 10 09:09:38.659 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2432 samples for epoch #23 from 2 peers. ETA 1.92 sec (refresh in 0.77 sec)\n",
            "Dec 10 09:09:39.626 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3104 samples for epoch #23 from 2 peers. ETA 0.97 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:40.323 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3552 samples for epoch #23 from 2 peers. ETA 0.30 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:41.018 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #24 from 2 peers. ETA 6.70 sec (refresh in 2.68 sec)\n",
            "Dec 10 09:09:42.899 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:09:42.902 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:09:42.908 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 24\n",
            "Dec 10 09:09:42.959 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:09:43.902 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 480 samples for epoch #24 from 2 peers. ETA 7.95 sec (refresh in 3.18 sec)\n",
            "Dec 10 09:09:47.289 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2592 samples for epoch #24 from 2 peers. ETA 2.82 sec (refresh in 1.13 sec)\n",
            "Dec 10 09:09:48.614 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3296 samples for epoch #24 from 2 peers. ETA 1.56 sec (refresh in 0.62 sec)\n",
            "Dec 10 09:09:49.442 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3648 samples for epoch #24 from 2 peers. ETA 0.78 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:50.150 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4032 samples for epoch #24 from 2 peers. ETA 0.04 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:50.851 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #25 from 2 peers. ETA 7.71 sec (refresh in 3.08 sec)\n",
            "Dec 10 09:09:52.830 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:09:52.833 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:09:52.838 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 25\n",
            "Dec 10 09:09:52.881 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:09:54.138 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 800 samples for epoch #25 from 2 peers. ETA 6.35 sec (refresh in 2.54 sec)\n",
            "Dec 10 09:09:56.879 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2592 samples for epoch #25 from 2 peers. ETA 2.81 sec (refresh in 1.12 sec)\n",
            "Dec 10 09:09:58.213 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3520 samples for epoch #25 from 2 peers. ETA 1.26 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:58.934 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3968 samples for epoch #25 from 2 peers. ETA 0.67 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:09:59.639 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4480 samples for epoch #25 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:00.338 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #26 from 2 peers. ETA 6.25 sec (refresh in 2.50 sec)\n",
            "Dec 10 09:10:02.589 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:02.591 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:02.595 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 26\n",
            "Dec 10 09:10:02.683 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:03.071 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #26 from 2 peers. ETA 11.28 sec (refresh in 4.51 sec)\n",
            "Dec 10 09:10:07.776 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2880 samples for epoch #26 from 2 peers. ETA 2.30 sec (refresh in 0.92 sec)\n",
            "Dec 10 09:10:08.896 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3616 samples for epoch #26 from 2 peers. ETA 1.15 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:09.597 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4064 samples for epoch #26 from 2 peers. ETA 0.50 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:10.292 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4512 samples for epoch #26 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:10.989 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #27 from 2 peers. ETA 7.14 sec (refresh in 2.86 sec)\n",
            "Dec 10 09:10:12.391 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:12.397 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:12.399 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 27\n",
            "Dec 10 09:10:12.455 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:14.070 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 928 samples for epoch #27 from 2 peers. ETA 6.22 sec (refresh in 2.49 sec)\n",
            "Dec 10 09:10:16.763 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2368 samples for epoch #27 from 2 peers. ETA 3.66 sec (refresh in 1.46 sec)\n",
            "Dec 10 09:10:18.432 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3200 samples for epoch #27 from 2 peers. ETA 1.88 sec (refresh in 0.75 sec)\n",
            "Dec 10 09:10:19.381 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3648 samples for epoch #27 from 2 peers. ETA 1.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:20.095 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4000 samples for epoch #27 from 2 peers. ETA 0.26 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:20.803 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #28 from 2 peers. ETA 8.51 sec (refresh in 3.40 sec)\n",
            "Dec 10 09:10:22.034 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:22.036 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:22.052 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 28\n",
            "Dec 10 09:10:22.091 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:24.408 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1408 samples for epoch #28 from 2 peers. ETA 4.91 sec (refresh in 1.97 sec)\n",
            "Dec 10 09:10:26.575 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2816 samples for epoch #28 from 2 peers. ETA 2.66 sec (refresh in 1.06 sec)\n",
            "Dec 10 09:10:27.849 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3712 samples for epoch #28 from 2 peers. ETA 1.32 sec (refresh in 0.53 sec)\n",
            "Dec 10 09:10:28.588 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4192 samples for epoch #28 from 2 peers. ETA 0.59 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:29.291 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4672 samples for epoch #28 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:29.988 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #29 from 2 peers. ETA 6.99 sec (refresh in 2.80 sec)\n",
            "Dec 10 09:10:31.820 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:31.823 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:31.836 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 29\n",
            "Dec 10 09:10:31.885 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:32.979 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 448 samples for epoch #29 from 2 peers. ETA 9.04 sec (refresh in 3.61 sec)\n",
            "Dec 10 09:10:36.789 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #29 from 2 peers. ETA 2.62 sec (refresh in 1.05 sec)\n",
            "Dec 10 09:10:38.047 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3328 samples for epoch #29 from 2 peers. ETA 1.36 sec (refresh in 0.54 sec)\n",
            "Dec 10 09:10:38.787 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3840 samples for epoch #29 from 2 peers. ETA 0.61 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:39.483 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4288 samples for epoch #29 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:40.179 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #30 from 2 peers. ETA 7.00 sec (refresh in 2.80 sec)\n",
            "Dec 10 09:10:41.609 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:41.612 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:41.625 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 30\n",
            "Dec 10 09:10:41.687 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:43.197 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 864 samples for epoch #30 from 2 peers. ETA 6.42 sec (refresh in 2.57 sec)\n",
            "Dec 10 09:10:45.967 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2624 samples for epoch #30 from 2 peers. ETA 3.18 sec (refresh in 1.27 sec)\n",
            "Dec 10 09:10:47.440 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3392 samples for epoch #30 from 2 peers. ETA 1.96 sec (refresh in 0.78 sec)\n",
            "Dec 10 09:10:48.438 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3840 samples for epoch #30 from 2 peers. ETA 1.03 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:49.154 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4192 samples for epoch #30 from 2 peers. ETA 0.31 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:49.871 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #31 from 2 peers. ETA 8.25 sec (refresh in 3.30 sec)\n",
            "Dec 10 09:10:51.383 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:10:51.390 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:10:51.399 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 31\n",
            "Dec 10 09:10:51.462 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:10:53.373 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1216 samples for epoch #31 from 2 peers. ETA 5.61 sec (refresh in 2.25 sec)\n",
            "Dec 10 09:10:55.820 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2816 samples for epoch #31 from 2 peers. ETA 2.90 sec (refresh in 1.16 sec)\n",
            "Dec 10 09:10:57.181 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3744 samples for epoch #31 from 2 peers. ETA 1.43 sec (refresh in 0.57 sec)\n",
            "Dec 10 09:10:57.961 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4224 samples for epoch #31 from 2 peers. ETA 0.73 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:58.656 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4672 samples for epoch #31 from 2 peers. ETA 0.06 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:10:59.351 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #32 from 2 peers. ETA 6.91 sec (refresh in 2.76 sec)\n",
            "Dec 10 09:11:01.146 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:11:01.150 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:11:01.153 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 32\n",
            "Dec 10 09:11:01.211 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:11:02.314 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 416 samples for epoch #32 from 2 peers. ETA 9.68 sec (refresh in 3.87 sec)\n",
            "Dec 10 09:11:06.392 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2464 samples for epoch #32 from 2 peers. ETA 3.02 sec (refresh in 1.21 sec)\n",
            "Dec 10 09:11:07.799 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3392 samples for epoch #32 from 2 peers. ETA 1.37 sec (refresh in 0.55 sec)\n",
            "Dec 10 09:11:08.545 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3872 samples for epoch #32 from 2 peers. ETA 0.68 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:11:09.243 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4352 samples for epoch #32 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:11:09.937 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #33 from 2 peers. ETA 7.31 sec (refresh in 2.92 sec)\n",
            "Dec 10 09:11:10.806 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:11:10.809 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:11:10.815 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 33\n",
            "Dec 10 09:11:10.864 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:11:13.056 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 96 samples for epoch #33 from 2 peers. ETA 9.53 sec (refresh in 3.81 sec)\n",
            "Dec 10 09:11:17.066 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 96 samples for epoch #33 from 2 peers. ETA 5.52 sec (refresh in 2.21 sec)\n",
            "Dec 10 09:11:19.472 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 96 samples for epoch #33 from 2 peers. ETA 3.12 sec (refresh in 1.25 sec)\n",
            "Dec 10 09:11:20.842 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:11:20.846 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:11:20.916 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 96 samples for epoch #33 from 2 peers. ETA 1.67 sec (refresh in 0.67 sec)\n",
            "Dec 10 09:11:21.216 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:11:21.793 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 736 samples for epoch #33 from 2 peers. ETA 22.05 sec (refresh in 8.82 sec)\n",
            "Dec 10 09:11:30.827 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8832 samples for epoch #33 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:11:30.852 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 34\n",
            "Dec 10 09:11:30.921 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:11:31.524 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #34 from 2 peers. ETA 9.59 sec (refresh in 3.84 sec)\n",
            "Dec 10 09:11:35.559 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #34 from 2 peers. ETA 6.20 sec (refresh in 2.48 sec)\n",
            "Dec 10 09:11:38.236 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #34 from 2 peers. ETA 3.95 sec (refresh in 1.58 sec)\n",
            "Dec 10 09:11:40.044 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #34 from 2 peers. ETA 2.42 sec (refresh in 0.97 sec)\n",
            "Dec 10 09:11:40.868 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:11:40.872 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:11:41.238 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #34 from 2 peers. ETA 49.46 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:11:41.263 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:11:51.490 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7936 samples for epoch #34 from 2 peers. ETA 1.73 sec (refresh in 0.69 sec)\n",
            "Dec 10 09:11:52.395 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8736 samples for epoch #34 from 2 peers. ETA 1.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:11:53.094 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9344 samples for epoch #34 from 2 peers. ETA 0.47 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:11:53.577 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 35\n",
            "Dec 10 09:11:53.640 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:11:53.883 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #35 from 2 peers. ETA 9.84 sec (refresh in 3.93 sec)\n",
            "Dec 10 09:11:58.014 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #35 from 2 peers. ETA 6.17 sec (refresh in 2.47 sec)\n",
            "Dec 10 09:12:00.694 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #35 from 2 peers. ETA 3.91 sec (refresh in 1.56 sec)\n",
            "Dec 10 09:12:02.453 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #35 from 2 peers. ETA 2.42 sec (refresh in 0.97 sec)\n",
            "Dec 10 09:12:03.600 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:12:03.603 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:12:03.622 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #35 from 2 peers. ETA 1.44 sec (refresh in 0.57 sec)\n",
            "Dec 10 09:12:03.967 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:12:04.393 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 576 samples for epoch #35 from 2 peers. ETA 31.11 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:12:14.604 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8928 samples for epoch #35 from 2 peers. ETA 1.17 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:15.333 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9312 samples for epoch #35 from 2 peers. ETA 0.76 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:16.078 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9728 samples for epoch #35 from 2 peers. ETA 0.20 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:16.290 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 36\n",
            "Dec 10 09:12:16.362 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:12:16.771 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #36 from 2 peers. ETA 13.70 sec (refresh in 5.48 sec)\n",
            "Dec 10 09:12:23.032 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #36 from 1 peers. ETA 11.32 sec (refresh in 2.83 sec)\n",
            "Dec 10 09:12:26.066 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2272 samples for epoch #36 from 2 peers. ETA 1.68 sec (refresh in 0.67 sec)\n",
            "Dec 10 09:12:26.305 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'concurrent.futures._base.TimeoutError'>\n",
            "Dec 10 09:12:26.310 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:12:26.712 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:12:27.015 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3200 samples for epoch #36 from 2 peers. ETA 8.04 sec (refresh in 3.22 sec)\n",
            "Dec 10 09:12:30.458 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7360 samples for epoch #36 from 2 peers. ETA 2.02 sec (refresh in 0.81 sec)\n",
            "Dec 10 09:12:31.532 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #36 from 2 peers. ETA 0.86 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:32.241 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9440 samples for epoch #36 from 2 peers. ETA 0.17 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:32.439 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 37\n",
            "Dec 10 09:12:32.501 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:12:32.941 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #37 from 2 peers. ETA 7.83 sec (refresh in 3.13 sec)\n",
            "Dec 10 09:12:36.196 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:12:36.200 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:12:36.284 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #37 from 2 peers. ETA 4.49 sec (refresh in 1.80 sec)\n",
            "Dec 10 09:12:38.353 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2784 samples for epoch #37 from 2 peers. ETA 5.08 sec (refresh in 2.03 sec)\n",
            "Dec 10 09:12:40.629 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6016 samples for epoch #37 from 2 peers. ETA 2.42 sec (refresh in 0.97 sec)\n",
            "Dec 10 09:12:41.808 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7680 samples for epoch #37 from 2 peers. ETA 1.34 sec (refresh in 0.54 sec)\n",
            "Dec 10 09:12:42.564 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8672 samples for epoch #37 from 2 peers. ETA 0.69 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:43.274 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9504 samples for epoch #37 from 2 peers. ETA 0.10 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:12:43.391 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 38\n",
            "Dec 10 09:12:43.504 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:12:43.972 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #38 from 2 peers. ETA 8.13 sec (refresh in 3.25 sec)\n",
            "Dec 10 09:12:47.064 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:12:47.074 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:12:47.752 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 352 samples for epoch #38 from 2 peers. ETA 45.45 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:12:58.551 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #39 from 2 peers. ETA 8.23 sec (refresh in 3.29 sec)\n",
            "Dec 10 09:12:58.784 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 39\n",
            "Dec 10 09:12:59.201 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:13:00.984 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:13:00.987 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:13:02.089 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 672 samples for epoch #39 from 2 peers. ETA 39.56 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:13:12.309 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #40 from 2 peers. ETA 5.59 sec (refresh in 2.24 sec)\n",
            "Dec 10 09:13:12.326 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 40\n",
            "Dec 10 09:13:12.372 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:13:14.747 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #40 from 2 peers. ETA 3.27 sec (refresh in 1.31 sec)\n",
            "Dec 10 09:13:15.817 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:13:15.821 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:13:16.335 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 320 samples for epoch #40 from 2 peers. ETA 55.45 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:13:26.603 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 14176 samples for epoch #40 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:26.616 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 41\n",
            "Dec 10 09:13:26.673 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:13:27.302 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #41 from 2 peers. ETA 7.62 sec (refresh in 3.05 sec)\n",
            "Dec 10 09:13:30.237 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:13:30.241 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:13:30.555 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 128 samples for epoch #41 from 2 peers. ETA 15.17 sec (refresh in 6.07 sec)\n",
            "Dec 10 09:13:36.852 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7712 samples for epoch #41 from 2 peers. ETA 1.33 sec (refresh in 0.53 sec)\n",
            "Dec 10 09:13:37.610 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8800 samples for epoch #41 from 2 peers. ETA 0.55 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:38.341 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9792 samples for epoch #41 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:38.344 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 42\n",
            "Dec 10 09:13:38.407 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:13:39.037 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #42 from 2 peers. ETA 6.98 sec (refresh in 2.79 sec)\n",
            "Dec 10 09:13:41.883 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:13:41.890 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:13:42.032 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #42 from 2 peers. ETA 3.99 sec (refresh in 1.60 sec)\n",
            "Dec 10 09:13:43.839 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2592 samples for epoch #42 from 2 peers. ETA 5.87 sec (refresh in 2.35 sec)\n",
            "Dec 10 09:13:46.427 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6016 samples for epoch #42 from 2 peers. ETA 2.96 sec (refresh in 1.18 sec)\n",
            "Dec 10 09:13:47.849 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7392 samples for epoch #42 from 2 peers. ETA 2.46 sec (refresh in 0.99 sec)\n",
            "Dec 10 09:13:49.090 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #42 from 2 peers. ETA 1.12 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:49.805 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9248 samples for epoch #42 from 2 peers. ETA 0.55 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:50.512 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9856 samples for epoch #42 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:13:50.514 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 43\n",
            "Dec 10 09:13:50.576 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:13:51.249 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #43 from 2 peers. ETA 9.87 sec (refresh in 3.95 sec)\n",
            "Dec 10 09:13:54.164 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:13:54.171 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:13:55.429 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1312 samples for epoch #43 from 2 peers. ETA 12.30 sec (refresh in 4.92 sec)\n",
            "Dec 10 09:14:00.609 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7936 samples for epoch #43 from 2 peers. ETA 1.11 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:01.351 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8800 samples for epoch #43 from 2 peers. ETA 0.65 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:02.099 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9728 samples for epoch #43 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:02.104 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 44\n",
            "Dec 10 09:14:02.204 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:14:02.805 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #44 from 2 peers. ETA 7.91 sec (refresh in 3.16 sec)\n",
            "Dec 10 09:14:05.806 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:14:05.812 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:14:06.189 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 288 samples for epoch #44 from 2 peers. ETA 48.97 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:14:16.403 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #45 from 2 peers. ETA 10.28 sec (refresh in 4.11 sec)\n",
            "Dec 10 09:14:16.441 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 45\n",
            "Dec 10 09:14:16.538 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:14:17.712 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:14:17.714 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:14:20.730 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4032 samples for epoch #45 from 2 peers. ETA 3.95 sec (refresh in 1.58 sec)\n",
            "Dec 10 09:14:22.561 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6656 samples for epoch #45 from 2 peers. ETA 1.96 sec (refresh in 0.78 sec)\n",
            "Dec 10 09:14:23.574 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8128 samples for epoch #45 from 2 peers. ETA 0.96 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:24.282 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9184 samples for epoch #45 from 2 peers. ETA 0.28 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:24.571 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 46\n",
            "Dec 10 09:14:24.624 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:14:24.987 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #46 from 2 peers. ETA 7.56 sec (refresh in 3.02 sec)\n",
            "Dec 10 09:14:28.178 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:14:28.187 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:14:28.216 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #46 from 2 peers. ETA 5.78 sec (refresh in 2.31 sec)\n",
            "Dec 10 09:14:30.763 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2688 samples for epoch #46 from 2 peers. ETA 6.13 sec (refresh in 2.45 sec)\n",
            "Dec 10 09:14:33.501 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5952 samples for epoch #46 from 2 peers. ETA 3.05 sec (refresh in 1.22 sec)\n",
            "Dec 10 09:14:35.137 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8064 samples for epoch #46 from 2 peers. ETA 0.89 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:35.858 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9312 samples for epoch #46 from 2 peers. ETA 0.19 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:36.085 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 47\n",
            "Dec 10 09:14:36.125 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:14:36.557 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #47 from 2 peers. ETA 6.76 sec (refresh in 2.70 sec)\n",
            "Dec 10 09:14:39.457 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #47 from 2 peers. ETA 3.86 sec (refresh in 1.54 sec)\n",
            "Dec 10 09:14:39.695 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:14:39.701 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:14:41.218 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1440 samples for epoch #47 from 2 peers. ETA 10.97 sec (refresh in 4.39 sec)\n",
            "Dec 10 09:14:45.866 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6912 samples for epoch #47 from 2 peers. ETA 2.28 sec (refresh in 0.91 sec)\n",
            "Dec 10 09:14:47.035 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8288 samples for epoch #47 from 2 peers. ETA 1.15 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:47.773 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9120 samples for epoch #47 from 2 peers. ETA 0.45 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:48.280 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 48\n",
            "Dec 10 09:14:48.375 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:14:48.561 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #48 from 2 peers. ETA 8.62 sec (refresh in 3.45 sec)\n",
            "Dec 10 09:14:51.893 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:14:51.896 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:14:52.304 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 224 samples for epoch #48 from 2 peers. ETA 9.14 sec (refresh in 3.66 sec)\n",
            "Dec 10 09:14:56.222 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5536 samples for epoch #48 from 2 peers. ETA 3.19 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:14:57.766 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7392 samples for epoch #48 from 2 peers. ETA 1.81 sec (refresh in 0.72 sec)\n",
            "Dec 10 09:14:58.734 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #48 from 2 peers. ETA 1.04 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:59.530 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9280 samples for epoch #48 from 2 peers. ETA 0.42 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:14:59.999 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 49\n",
            "Dec 10 09:15:00.050 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:15:00.312 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #49 from 2 peers. ETA 9.63 sec (refresh in 3.85 sec)\n",
            "Dec 10 09:15:04.362 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #49 from 2 peers. ETA 4.85 sec (refresh in 1.94 sec)\n",
            "Dec 10 09:15:06.510 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #49 from 2 peers. ETA 2.71 sec (refresh in 1.08 sec)\n",
            "Dec 10 09:15:06.660 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:15:06.667 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:15:07.829 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1408 samples for epoch #49 from 2 peers. ETA 13.62 sec (refresh in 5.45 sec)\n",
            "Dec 10 09:15:13.535 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8608 samples for epoch #49 from 2 peers. ETA 1.04 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:15:14.284 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9376 samples for epoch #49 from 2 peers. ETA 0.31 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:15:14.644 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 50\n",
            "Dec 10 09:15:14.735 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:15:15.027 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #50 from 2 peers. ETA 10.43 sec (refresh in 4.17 sec)\n",
            "Dec 10 09:15:19.397 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #50 from 2 peers. ETA 5.10 sec (refresh in 2.04 sec)\n",
            "Dec 10 09:15:21.211 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:15:21.216 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:15:21.756 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 544 samples for epoch #50 from 2 peers. ETA 41.39 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:15:31.963 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #51 from 2 peers. ETA 9.17 sec (refresh in 3.67 sec)\n",
            "Dec 10 09:15:31.990 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 51\n",
            "Dec 10 09:15:32.060 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:15:35.695 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:15:35.697 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:15:35.842 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #51 from 2 peers. ETA 4.28 sec (refresh in 1.71 sec)\n",
            "Dec 10 09:15:37.767 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2816 samples for epoch #51 from 2 peers. ETA 5.32 sec (refresh in 2.13 sec)\n",
            "Dec 10 09:15:40.119 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6240 samples for epoch #51 from 2 peers. ETA 2.29 sec (refresh in 0.91 sec)\n",
            "Dec 10 09:15:41.280 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7808 samples for epoch #51 from 2 peers. ETA 1.28 sec (refresh in 0.51 sec)\n",
            "Dec 10 09:15:42.031 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8640 samples for epoch #51 from 2 peers. ETA 0.89 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:15:42.768 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9280 samples for epoch #51 from 2 peers. ETA 0.44 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:15:43.217 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 52\n",
            "Dec 10 09:15:43.307 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:15:43.503 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #52 from 2 peers. ETA 10.33 sec (refresh in 4.13 sec)\n",
            "Dec 10 09:15:47.834 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #52 from 2 peers. ETA 6.77 sec (refresh in 2.71 sec)\n",
            "Dec 10 09:15:49.852 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:15:49.855 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:15:50.759 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 992 samples for epoch #52 from 2 peers. ETA 21.57 sec (refresh in 8.63 sec)\n",
            "Dec 10 09:15:59.637 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #53 from 2 peers. ETA 7.61 sec (refresh in 3.05 sec)\n",
            "Dec 10 09:15:59.652 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 53\n",
            "Dec 10 09:15:59.717 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:02.880 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #53 from 2 peers. ETA 4.33 sec (refresh in 1.73 sec)\n",
            "Dec 10 09:16:03.259 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:16:03.268 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:16:04.860 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1920 samples for epoch #53 from 2 peers. ETA 8.73 sec (refresh in 3.49 sec)\n",
            "Dec 10 09:16:08.570 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7392 samples for epoch #53 from 2 peers. ETA 1.51 sec (refresh in 0.60 sec)\n",
            "Dec 10 09:16:09.386 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #53 from 2 peers. ETA 0.73 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:10.135 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9632 samples for epoch #53 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:10.174 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 54\n",
            "Dec 10 09:16:10.228 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:10.855 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #54 from 2 peers. ETA 7.49 sec (refresh in 3.00 sec)\n",
            "Dec 10 09:16:14.075 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #54 from 2 peers. ETA 5.49 sec (refresh in 2.20 sec)\n",
            "Dec 10 09:16:16.467 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #54 from 2 peers. ETA 3.19 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:16:16.776 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:16:16.778 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:16:17.954 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1344 samples for epoch #54 from 2 peers. ETA 13.84 sec (refresh in 5.54 sec)\n",
            "Dec 10 09:16:23.713 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9696 samples for epoch #54 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:23.720 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 55\n",
            "Dec 10 09:16:23.772 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:24.415 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #55 from 2 peers. ETA 7.76 sec (refresh in 3.10 sec)\n",
            "Dec 10 09:16:27.391 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:16:27.398 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:16:27.791 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #55 from 2 peers. ETA 9.54 sec (refresh in 3.82 sec)\n",
            "Dec 10 09:16:31.853 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5920 samples for epoch #55 from 2 peers. ETA 2.64 sec (refresh in 1.06 sec)\n",
            "Dec 10 09:16:33.151 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7808 samples for epoch #55 from 2 peers. ETA 1.21 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:33.907 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8736 samples for epoch #55 from 2 peers. ETA 0.56 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:34.754 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 10016 samples for epoch #55 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:34.757 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 56\n",
            "Dec 10 09:16:34.807 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:35.453 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #56 from 2 peers. ETA 5.93 sec (refresh in 2.37 sec)\n",
            "Dec 10 09:16:38.022 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #56 from 2 peers. ETA 3.36 sec (refresh in 1.34 sec)\n",
            "Dec 10 09:16:38.137 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:16:38.140 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:16:39.624 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1152 samples for epoch #56 from 2 peers. ETA 17.38 sec (refresh in 6.95 sec)\n",
            "Dec 10 09:16:46.779 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 10560 samples for epoch #56 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:46.815 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 57\n",
            "Dec 10 09:16:46.861 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:47.478 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #57 from 2 peers. ETA 6.76 sec (refresh in 2.71 sec)\n",
            "Dec 10 09:16:50.382 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #57 from 2 peers. ETA 4.33 sec (refresh in 1.73 sec)\n",
            "Dec 10 09:16:50.423 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:16:50.429 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:16:52.376 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1952 samples for epoch #57 from 2 peers. ETA 8.01 sec (refresh in 3.20 sec)\n",
            "Dec 10 09:16:55.835 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5504 samples for epoch #57 from 2 peers. ETA 3.78 sec (refresh in 1.51 sec)\n",
            "Dec 10 09:16:57.582 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7680 samples for epoch #57 from 2 peers. ETA 1.62 sec (refresh in 0.65 sec)\n",
            "Dec 10 09:16:58.451 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8640 samples for epoch #57 from 2 peers. ETA 0.94 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:59.170 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9504 samples for epoch #57 from 2 peers. ETA 0.17 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:16:59.358 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 58\n",
            "Dec 10 09:16:59.412 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:16:59.867 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #58 from 2 peers. ETA 7.40 sec (refresh in 2.96 sec)\n",
            "Dec 10 09:17:03.018 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:17:03.021 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:17:03.098 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #58 from 2 peers. ETA 4.17 sec (refresh in 1.67 sec)\n",
            "Dec 10 09:17:04.979 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #58 from 2 peers. ETA 5.95 sec (refresh in 2.38 sec)\n",
            "Dec 10 09:17:07.626 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5664 samples for epoch #58 from 2 peers. ETA 3.21 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:17:09.128 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7456 samples for epoch #58 from 2 peers. ETA 1.87 sec (refresh in 0.75 sec)\n",
            "Dec 10 09:17:10.195 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8320 samples for epoch #58 from 2 peers. ETA 1.29 sec (refresh in 0.52 sec)\n",
            "Dec 10 09:17:10.922 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9216 samples for epoch #58 from 2 peers. ETA 0.37 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:17:11.336 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 59\n",
            "Dec 10 09:17:11.388 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:17:11.623 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #59 from 2 peers. ETA 8.05 sec (refresh in 3.22 sec)\n",
            "Dec 10 09:17:14.929 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:17:14.933 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:17:15.039 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #59 from 2 peers. ETA 4.75 sec (refresh in 1.90 sec)\n",
            "Dec 10 09:17:17.235 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2848 samples for epoch #59 from 2 peers. ETA 5.33 sec (refresh in 2.13 sec)\n",
            "Dec 10 09:17:19.592 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6464 samples for epoch #59 from 2 peers. ETA 2.06 sec (refresh in 0.82 sec)\n",
            "Dec 10 09:17:20.631 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7936 samples for epoch #59 from 2 peers. ETA 1.08 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:17:21.365 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8768 samples for epoch #59 from 2 peers. ETA 0.74 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:17:22.146 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9696 samples for epoch #59 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:17:22.151 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 60\n",
            "Dec 10 09:17:22.254 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:17:22.852 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #60 from 2 peers. ETA 8.64 sec (refresh in 3.45 sec)\n",
            "Dec 10 09:17:25.812 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:17:25.815 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:17:26.626 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 832 samples for epoch #60 from 2 peers. ETA 22.29 sec (refresh in 8.92 sec)\n",
            "Dec 10 09:17:35.786 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 13888 samples for epoch #60 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:17:35.839 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 61\n",
            "Dec 10 09:17:35.928 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:17:36.492 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #61 from 2 peers. ETA 9.75 sec (refresh in 3.90 sec)\n",
            "Dec 10 09:17:39.714 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:17:39.723 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:17:40.614 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 928 samples for epoch #61 from 2 peers. ETA 18.91 sec (refresh in 7.57 sec)\n",
            "Dec 10 09:17:48.383 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #62 from 2 peers. ETA 6.79 sec (refresh in 2.72 sec)\n",
            "Dec 10 09:17:48.425 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 62\n",
            "Dec 10 09:17:48.559 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:17:50.372 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:17:50.375 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:17:51.318 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 736 samples for epoch #62 from 2 peers. ETA 21.62 sec (refresh in 8.65 sec)\n",
            "Dec 10 09:18:00.276 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #63 from 2 peers. ETA 6.00 sec (refresh in 2.40 sec)\n",
            "Dec 10 09:18:00.305 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 63\n",
            "Dec 10 09:18:00.352 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:01.571 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:18:01.574 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:18:02.939 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1440 samples for epoch #63 from 2 peers. ETA 9.66 sec (refresh in 3.86 sec)\n",
            "Dec 10 09:18:07.014 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5408 samples for epoch #63 from 2 peers. ETA 4.60 sec (refresh in 1.84 sec)\n",
            "Dec 10 09:18:09.090 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7872 samples for epoch #63 from 2 peers. ETA 1.44 sec (refresh in 0.58 sec)\n",
            "Dec 10 09:18:09.913 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8960 samples for epoch #63 from 2 peers. ETA 0.57 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:10.614 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9952 samples for epoch #63 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:10.616 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 64\n",
            "Dec 10 09:18:10.684 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:11.310 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #64 from 2 peers. ETA 7.04 sec (refresh in 2.82 sec)\n",
            "Dec 10 09:18:14.225 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:18:14.229 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:18:14.329 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #64 from 2 peers. ETA 4.03 sec (refresh in 1.61 sec)\n",
            "Dec 10 09:18:16.178 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2528 samples for epoch #64 from 2 peers. ETA 5.84 sec (refresh in 2.34 sec)\n",
            "Dec 10 09:18:18.756 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5856 samples for epoch #64 from 2 peers. ETA 3.28 sec (refresh in 1.31 sec)\n",
            "Dec 10 09:18:20.349 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7680 samples for epoch #64 from 2 peers. ETA 1.81 sec (refresh in 0.72 sec)\n",
            "Dec 10 09:18:21.320 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8640 samples for epoch #64 from 2 peers. ETA 1.12 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:22.052 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9312 samples for epoch #64 from 2 peers. ETA 0.44 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:22.531 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 65\n",
            "Dec 10 09:18:22.616 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:22.800 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #65 from 2 peers. ETA 10.45 sec (refresh in 4.18 sec)\n",
            "Dec 10 09:18:26.139 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:18:26.145 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:18:27.211 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1184 samples for epoch #65 from 2 peers. ETA 14.41 sec (refresh in 5.76 sec)\n",
            "Dec 10 09:18:33.206 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9728 samples for epoch #65 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:33.267 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 66\n",
            "Dec 10 09:18:33.359 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:33.908 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #66 from 2 peers. ETA 8.36 sec (refresh in 3.34 sec)\n",
            "Dec 10 09:18:36.733 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:18:36.744 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:18:37.463 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 832 samples for epoch #66 from 2 peers. ETA 21.66 sec (refresh in 8.66 sec)\n",
            "Dec 10 09:18:46.378 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #67 from 2 peers. ETA 6.49 sec (refresh in 2.59 sec)\n",
            "Dec 10 09:18:46.393 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 67\n",
            "Dec 10 09:18:46.496 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:49.175 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #67 from 2 peers. ETA 4.42 sec (refresh in 1.77 sec)\n",
            "Dec 10 09:18:49.985 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:18:49.990 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:18:51.148 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1344 samples for epoch #67 from 2 peers. ETA 13.69 sec (refresh in 5.48 sec)\n",
            "Dec 10 09:18:56.839 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9568 samples for epoch #67 from 2 peers. ETA 0.03 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:18:56.906 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 68\n",
            "Dec 10 09:18:56.953 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:18:57.534 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #68 from 2 peers. ETA 7.19 sec (refresh in 2.88 sec)\n",
            "Dec 10 09:19:00.519 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:00.523 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:00.629 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #68 from 2 peers. ETA 4.10 sec (refresh in 1.64 sec)\n",
            "Dec 10 09:19:02.488 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1568 samples for epoch #68 from 2 peers. ETA 13.13 sec (refresh in 5.25 sec)\n",
            "Dec 10 09:19:07.956 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8960 samples for epoch #68 from 2 peers. ETA 0.38 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:08.377 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 69\n",
            "Dec 10 09:19:08.441 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:19:08.665 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #69 from 2 peers. ETA 6.78 sec (refresh in 2.71 sec)\n",
            "Dec 10 09:19:11.577 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #69 from 2 peers. ETA 4.04 sec (refresh in 1.62 sec)\n",
            "Dec 10 09:19:12.015 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:12.018 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:13.430 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1504 samples for epoch #69 from 2 peers. ETA 11.23 sec (refresh in 4.49 sec)\n",
            "Dec 10 09:19:18.138 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6432 samples for epoch #69 from 2 peers. ETA 3.64 sec (refresh in 1.46 sec)\n",
            "Dec 10 09:19:19.978 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8256 samples for epoch #69 from 2 peers. ETA 1.17 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:20.706 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9312 samples for epoch #69 from 2 peers. ETA 0.29 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:21.012 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 70\n",
            "Dec 10 09:19:21.090 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:19:21.404 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #70 from 2 peers. ETA 7.69 sec (refresh in 3.08 sec)\n",
            "Dec 10 09:19:24.633 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:24.637 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:24.683 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #70 from 2 peers. ETA 4.29 sec (refresh in 1.72 sec)\n",
            "Dec 10 09:19:26.610 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #70 from 2 peers. ETA 5.68 sec (refresh in 2.27 sec)\n",
            "Dec 10 09:19:29.116 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6176 samples for epoch #70 from 2 peers. ETA 2.31 sec (refresh in 0.93 sec)\n",
            "Dec 10 09:19:30.281 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7520 samples for epoch #70 from 2 peers. ETA 1.71 sec (refresh in 0.68 sec)\n",
            "Dec 10 09:19:31.206 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #70 from 2 peers. ETA 1.13 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:31.909 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9152 samples for epoch #70 from 2 peers. ETA 0.59 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:32.662 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9856 samples for epoch #70 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:32.673 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 71\n",
            "Dec 10 09:19:32.749 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:19:33.472 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #71 from 2 peers. ETA 9.60 sec (refresh in 3.84 sec)\n",
            "Dec 10 09:19:36.290 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:36.295 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:37.556 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1440 samples for epoch #71 from 2 peers. ETA 11.22 sec (refresh in 4.49 sec)\n",
            "Dec 10 09:19:42.256 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8384 samples for epoch #71 from 2 peers. ETA 0.88 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:42.990 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9408 samples for epoch #71 from 2 peers. ETA 0.13 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:43.136 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 72\n",
            "Dec 10 09:19:43.195 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:19:43.693 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #72 from 2 peers. ETA 6.87 sec (refresh in 2.75 sec)\n",
            "Dec 10 09:19:46.638 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #72 from 2 peers. ETA 3.93 sec (refresh in 1.57 sec)\n",
            "Dec 10 09:19:46.759 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:46.761 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:48.428 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1952 samples for epoch #72 from 2 peers. ETA 7.62 sec (refresh in 3.05 sec)\n",
            "Dec 10 09:19:51.724 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6784 samples for epoch #72 from 2 peers. ETA 1.91 sec (refresh in 0.76 sec)\n",
            "Dec 10 09:19:52.923 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8320 samples for epoch #72 from 2 peers. ETA 0.70 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:53.638 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9568 samples for epoch #72 from 2 peers. ETA 0.03 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:19:53.700 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 73\n",
            "Dec 10 09:19:53.747 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:19:54.336 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #73 from 2 peers. ETA 6.33 sec (refresh in 2.53 sec)\n",
            "Dec 10 09:19:57.112 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #73 from 2 peers. ETA 3.56 sec (refresh in 1.42 sec)\n",
            "Dec 10 09:19:57.502 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:19:57.505 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:19:58.768 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1024 samples for epoch #73 from 2 peers. ETA 20.07 sec (refresh in 8.03 sec)\n",
            "Dec 10 09:20:07.038 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 11424 samples for epoch #73 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:07.085 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 74\n",
            "Dec 10 09:20:07.151 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:20:07.742 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #74 from 2 peers. ETA 6.52 sec (refresh in 2.61 sec)\n",
            "Dec 10 09:20:10.545 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #74 from 2 peers. ETA 3.77 sec (refresh in 1.51 sec)\n",
            "Dec 10 09:20:10.684 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:20:10.687 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:20:12.307 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1536 samples for epoch #74 from 2 peers. ETA 10.57 sec (refresh in 4.23 sec)\n",
            "Dec 10 09:20:16.794 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5792 samples for epoch #74 from 2 peers. ETA 4.02 sec (refresh in 1.61 sec)\n",
            "Dec 10 09:20:18.617 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8128 samples for epoch #74 from 2 peers. ETA 1.30 sec (refresh in 0.52 sec)\n",
            "Dec 10 09:20:19.428 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9248 samples for epoch #74 from 2 peers. ETA 0.26 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:19.717 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 75\n",
            "Dec 10 09:20:19.762 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:20:20.127 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #75 from 2 peers. ETA 6.73 sec (refresh in 2.69 sec)\n",
            "Dec 10 09:20:23.160 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #75 from 2 peers. ETA 3.70 sec (refresh in 1.48 sec)\n",
            "Dec 10 09:20:23.451 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:20:23.455 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:20:24.859 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1792 samples for epoch #75 from 2 peers. ETA 8.72 sec (refresh in 3.49 sec)\n",
            "Dec 10 09:20:28.605 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6848 samples for epoch #75 from 2 peers. ETA 2.13 sec (refresh in 0.85 sec)\n",
            "Dec 10 09:20:29.688 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7872 samples for epoch #75 from 2 peers. ETA 1.74 sec (refresh in 0.70 sec)\n",
            "Dec 10 09:20:30.622 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8736 samples for epoch #75 from 2 peers. ETA 0.97 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:31.359 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9440 samples for epoch #75 from 2 peers. ETA 0.28 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:31.690 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 76\n",
            "Dec 10 09:20:31.771 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:20:32.058 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #76 from 2 peers. ETA 10.71 sec (refresh in 4.28 sec)\n",
            "Dec 10 09:20:35.397 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:20:35.402 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:20:36.581 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1536 samples for epoch #76 from 2 peers. ETA 9.98 sec (refresh in 3.99 sec)\n",
            "Dec 10 09:20:40.789 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7904 samples for epoch #76 from 2 peers. ETA 1.14 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:41.521 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8992 samples for epoch #76 from 2 peers. ETA 0.39 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:41.947 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 77\n",
            "Dec 10 09:20:42.008 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:20:42.261 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #77 from 2 peers. ETA 6.99 sec (refresh in 2.79 sec)\n",
            "Dec 10 09:20:45.256 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #77 from 2 peers. ETA 4.23 sec (refresh in 1.69 sec)\n",
            "Dec 10 09:20:45.570 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:20:45.572 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:20:47.167 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2048 samples for epoch #77 from 2 peers. ETA 6.80 sec (refresh in 2.72 sec)\n",
            "Dec 10 09:20:50.128 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6464 samples for epoch #77 from 2 peers. ETA 1.99 sec (refresh in 0.79 sec)\n",
            "Dec 10 09:20:51.166 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8064 samples for epoch #77 from 2 peers. ETA 0.93 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:51.887 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9152 samples for epoch #77 from 2 peers. ETA 0.32 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:20:52.231 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 78\n",
            "Dec 10 09:20:52.284 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:20:52.592 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #78 from 2 peers. ETA 6.52 sec (refresh in 2.61 sec)\n",
            "Dec 10 09:20:55.407 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #78 from 2 peers. ETA 3.71 sec (refresh in 1.48 sec)\n",
            "Dec 10 09:20:55.958 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaged parameters with 2 peers\n",
            "Dec 10 09:20:55.961 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:20:57.146 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1024 samples for epoch #78 from 2 peers. ETA 19.55 sec (refresh in 7.82 sec)\n",
            "Dec 10 09:21:05.005 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 10784 samples for epoch #78 from 2 peers. ETA 0.00 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:05.029 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 79\n",
            "Dec 10 09:21:05.090 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:21:05.513 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 7.58 sec (refresh in 3.03 sec)\n",
            "Dec 10 09:21:08.553 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 5.53 sec (refresh in 2.21 sec)\n",
            "Dec 10 09:21:10.769 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 4.03 sec (refresh in 1.61 sec)\n",
            "Dec 10 09:21:12.390 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 2.93 sec (refresh in 1.17 sec)\n",
            "Dec 10 09:21:13.568 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 2.13 sec (refresh in 0.85 sec)\n",
            "Dec 10 09:21:14.429 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 1.55 sec (refresh in 0.62 sec)\n",
            "Dec 10 09:21:15.036 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:21:15.040 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'hivemind.averaging.partition.AllreduceException'>\n",
            "Dec 10 09:21:15.044 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:21:15.061 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #79 from 2 peers. ETA 1.12 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:15.566 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 512 samples for epoch #79 from 2 peers. ETA 17.85 sec (refresh in 7.14 sec)\n",
            "Dec 10 09:21:22.711 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7968 samples for epoch #79 from 2 peers. ETA 1.40 sec (refresh in 0.56 sec)\n",
            "Dec 10 09:21:23.275 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8544 samples for epoch #79 from 2 peers. ETA 0.99 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:23.801 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8992 samples for epoch #79 from 2 peers. ETA 0.78 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:24.314 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9344 samples for epoch #79 from 2 peers. ETA 0.53 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:24.821 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9728 samples for epoch #79 from 2 peers. ETA 0.21 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:25.073 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 80\n",
            "Dec 10 09:21:25.171 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:21:25.329 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #80 from 2 peers. ETA 9.01 sec (refresh in 3.61 sec)\n",
            "Dec 10 09:21:28.941 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #80 from 2 peers. ETA 6.76 sec (refresh in 2.70 sec)\n",
            "Dec 10 09:21:31.653 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #80 from 2 peers. ETA 5.06 sec (refresh in 2.02 sec)\n",
            "Dec 10 09:21:33.682 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #80 from 2 peers. ETA 3.79 sec (refresh in 1.52 sec)\n",
            "Dec 10 09:21:35.065 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:21:35.070 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'hivemind.averaging.partition.AllreduceException'>\n",
            "Dec 10 09:21:35.073 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:21:35.215 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #80 from 2 peers. ETA 21.80 sec (refresh in 8.72 sec)\n",
            "Dec 10 09:21:43.947 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7648 samples for epoch #80 from 2 peers. ETA 1.69 sec (refresh in 0.68 sec)\n",
            "Dec 10 09:21:44.632 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8320 samples for epoch #80 from 2 peers. ETA 1.21 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:45.136 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8864 samples for epoch #80 from 2 peers. ETA 0.76 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:45.647 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9376 samples for epoch #80 from 2 peers. ETA 0.42 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:46.090 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 81\n",
            "Dec 10 09:21:46.142 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:21:46.157 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 7.12 sec (refresh in 2.85 sec)\n",
            "Dec 10 09:21:49.011 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 5.10 sec (refresh in 2.04 sec)\n",
            "Dec 10 09:21:51.063 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 3.65 sec (refresh in 1.46 sec)\n",
            "Dec 10 09:21:52.528 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 2.61 sec (refresh in 1.04 sec)\n",
            "Dec 10 09:21:53.579 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 1.87 sec (refresh in 0.75 sec)\n",
            "Dec 10 09:21:54.331 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 1.34 sec (refresh in 0.53 sec)\n",
            "Dec 10 09:21:54.873 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 0.95 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:55.380 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 0.59 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:55.890 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #81 from 2 peers. ETA 0.23 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:21:56.095 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:21:56.105 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'hivemind.averaging.partition.AllreduceException'>\n",
            "Dec 10 09:21:56.107 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:21:56.178 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 82\n",
            "Dec 10 09:21:56.267 [\u001b[1m\u001b[34mINFO\u001b[0m] Waiting for delayed updates to finish...\n",
            "Dec 10 09:21:56.398 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #82 from 2 peers. ETA 22.46 sec (refresh in 8.98 sec)\n",
            "Dec 10 09:22:05.390 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 32 samples for epoch #82 from 1 peers. ETA 282.43 sec (refresh in 10.00 sec)\n",
            "Dec 10 09:22:06.178 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:480\u001b[0m] Averaging step failed: could not find a group\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hivemind/averaging/averager.py\", line 450, in _step\n",
            "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
            "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n",
            "Dec 10 09:22:06.181 [\u001b[1m\u001b[34mINFO\u001b[0m] Averaging failed with <class 'hivemind.averaging.partition.AllreduceException'>\n",
            "Dec 10 09:22:06.185 [\u001b[1m\u001b[34mINFO\u001b[0m] Received parameters from background averaging round\n",
            "Dec 10 09:22:15.398 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8000 samples for epoch #82 from 1 peers. ETA 1.88 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:15.903 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8480 samples for epoch #82 from 1 peers. ETA 1.45 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:16.411 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9024 samples for epoch #82 from 1 peers. ETA 0.90 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:16.916 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9536 samples for epoch #82 from 1 peers. ETA 0.42 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:17.348 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 83\n",
            "Dec 10 09:22:17.421 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #83 from 1 peers. ETA 9.38 sec (refresh in 2.35 sec)\n",
            "Dec 10 09:22:19.782 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #83 from 1 peers. ETA 6.97 sec (refresh in 1.74 sec)\n",
            "Dec 10 09:22:21.530 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4320 samples for epoch #83 from 1 peers. ETA 5.58 sec (refresh in 1.40 sec)\n",
            "Dec 10 09:22:22.931 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5760 samples for epoch #83 from 1 peers. ETA 4.15 sec (refresh in 1.04 sec)\n",
            "Dec 10 09:22:23.986 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6560 samples for epoch #83 from 1 peers. ETA 4.58 sec (refresh in 1.15 sec)\n",
            "Dec 10 09:22:25.143 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7424 samples for epoch #83 from 1 peers. ETA 3.34 sec (refresh in 0.84 sec)\n",
            "Dec 10 09:22:25.992 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8032 samples for epoch #83 from 1 peers. ETA 2.59 sec (refresh in 0.65 sec)\n",
            "Dec 10 09:22:26.646 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #83 from 1 peers. ETA 2.06 sec (refresh in 0.52 sec)\n",
            "Dec 10 09:22:27.166 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8864 samples for epoch #83 from 1 peers. ETA 1.60 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:27.687 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9216 samples for epoch #83 from 1 peers. ETA 1.11 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:28.191 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9696 samples for epoch #83 from 1 peers. ETA 0.32 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:28.488 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 84\n",
            "Dec 10 09:22:28.696 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 160 samples for epoch #84 from 1 peers. ETA 10.45 sec (refresh in 2.61 sec)\n",
            "Dec 10 09:22:31.313 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2912 samples for epoch #84 from 1 peers. ETA 6.64 sec (refresh in 1.66 sec)\n",
            "Dec 10 09:22:32.979 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4608 samples for epoch #84 from 1 peers. ETA 5.05 sec (refresh in 1.26 sec)\n",
            "Dec 10 09:22:34.246 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5920 samples for epoch #84 from 1 peers. ETA 3.87 sec (refresh in 0.97 sec)\n",
            "Dec 10 09:22:35.217 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6944 samples for epoch #84 from 1 peers. ETA 2.81 sec (refresh in 0.70 sec)\n",
            "Dec 10 09:22:35.934 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7680 samples for epoch #84 from 1 peers. ETA 2.20 sec (refresh in 0.55 sec)\n",
            "Dec 10 09:22:36.490 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8224 samples for epoch #84 from 1 peers. ETA 1.78 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:36.997 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8736 samples for epoch #84 from 1 peers. ETA 1.22 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:37.505 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9248 samples for epoch #84 from 1 peers. ETA 0.71 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:38.019 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9728 samples for epoch #84 from 1 peers. ETA 0.26 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:38.326 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 85\n",
            "Dec 10 09:22:38.528 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 96 samples for epoch #85 from 1 peers. ETA 13.18 sec (refresh in 3.30 sec)\n",
            "Dec 10 09:22:41.833 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2496 samples for epoch #85 from 1 peers. ETA 10.50 sec (refresh in 2.62 sec)\n",
            "Dec 10 09:22:44.471 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5024 samples for epoch #85 from 1 peers. ETA 4.79 sec (refresh in 1.20 sec)\n",
            "Dec 10 09:22:45.688 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6208 samples for epoch #85 from 1 peers. ETA 3.72 sec (refresh in 0.93 sec)\n",
            "Dec 10 09:22:46.624 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7200 samples for epoch #85 from 1 peers. ETA 2.79 sec (refresh in 0.70 sec)\n",
            "Dec 10 09:22:47.326 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7936 samples for epoch #85 from 1 peers. ETA 1.95 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:47.835 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8448 samples for epoch #85 from 1 peers. ETA 1.50 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:48.342 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8992 samples for epoch #85 from 1 peers. ETA 0.98 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:48.847 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9504 samples for epoch #85 from 1 peers. ETA 0.46 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:49.334 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 86\n",
            "Dec 10 09:22:49.364 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 0 samples for epoch #86 from 1 peers. ETA 9.77 sec (refresh in 2.44 sec)\n",
            "Dec 10 09:22:51.817 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2528 samples for epoch #86 from 1 peers. ETA 7.28 sec (refresh in 1.82 sec)\n",
            "Dec 10 09:22:53.646 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4032 samples for epoch #86 from 1 peers. ETA 8.16 sec (refresh in 2.04 sec)\n",
            "Dec 10 09:22:55.690 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5504 samples for epoch #86 from 1 peers. ETA 6.36 sec (refresh in 1.59 sec)\n",
            "Dec 10 09:22:57.283 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6656 samples for epoch #86 from 1 peers. ETA 4.57 sec (refresh in 1.14 sec)\n",
            "Dec 10 09:22:58.432 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7712 samples for epoch #86 from 1 peers. ETA 2.19 sec (refresh in 0.55 sec)\n",
            "Dec 10 09:22:58.992 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8288 samples for epoch #86 from 1 peers. ETA 1.69 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:22:59.499 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8800 samples for epoch #86 from 1 peers. ETA 1.17 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:00.015 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9312 samples for epoch #86 from 1 peers. ETA 0.66 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:00.529 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9824 samples for epoch #86 from 1 peers. ETA 0.14 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:00.691 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 87\n",
            "Dec 10 09:23:01.035 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 320 samples for epoch #87 from 1 peers. ETA 9.42 sec (refresh in 2.36 sec)\n",
            "Dec 10 09:23:03.398 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 2784 samples for epoch #87 from 1 peers. ETA 6.79 sec (refresh in 1.70 sec)\n",
            "Dec 10 09:23:05.112 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 4288 samples for epoch #87 from 1 peers. ETA 6.97 sec (refresh in 1.74 sec)\n",
            "Dec 10 09:23:06.861 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5728 samples for epoch #87 from 1 peers. ETA 5.11 sec (refresh in 1.28 sec)\n",
            "Dec 10 09:23:08.169 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6656 samples for epoch #87 from 1 peers. ETA 4.79 sec (refresh in 1.20 sec)\n",
            "Dec 10 09:23:09.481 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6816 samples for epoch #87 from 1 peers. ETA 11.19 sec (refresh in 2.80 sec)\n",
            "Dec 10 09:23:12.313 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7456 samples for epoch #87 from 1 peers. ETA 8.87 sec (refresh in 2.22 sec)\n",
            "Dec 10 09:23:14.546 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8800 samples for epoch #87 from 1 peers. ETA 1.71 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:15.060 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9184 samples for epoch #87 from 1 peers. ETA 1.05 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:15.568 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9536 samples for epoch #87 from 1 peers. ETA 0.64 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:16.094 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9888 samples for epoch #87 from 1 peers. ETA 0.14 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:16.265 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 88\n",
            "Dec 10 09:23:16.602 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 224 samples for epoch #88 from 1 peers. ETA 13.61 sec (refresh in 3.40 sec)\n",
            "Dec 10 09:23:20.013 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3776 samples for epoch #88 from 1 peers. ETA 5.67 sec (refresh in 1.42 sec)\n",
            "Dec 10 09:23:21.436 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 5248 samples for epoch #88 from 1 peers. ETA 4.53 sec (refresh in 1.13 sec)\n",
            "Dec 10 09:23:22.577 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6400 samples for epoch #88 from 1 peers. ETA 3.55 sec (refresh in 0.89 sec)\n",
            "Dec 10 09:23:23.480 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7360 samples for epoch #88 from 1 peers. ETA 2.51 sec (refresh in 0.63 sec)\n",
            "Dec 10 09:23:24.123 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8000 samples for epoch #88 from 1 peers. ETA 1.86 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:24.630 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 8512 samples for epoch #88 from 1 peers. ETA 1.47 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:25.138 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9024 samples for epoch #88 from 1 peers. ETA 0.87 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:25.643 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 9536 samples for epoch #88 from 1 peers. ETA 0.44 sec (refresh in 0.50 sec)\n",
            "Dec 10 09:23:26.070 [\u001b[1m\u001b[34mINFO\u001b[0m] Transitioning to epoch 89\n",
            "Dec 10 09:23:26.151 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 64 samples for epoch #89 from 1 peers. ETA 9.49 sec (refresh in 2.37 sec)\n",
            "Dec 10 09:23:28.538 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 1888 samples for epoch #89 from 1 peers. ETA 11.08 sec (refresh in 2.77 sec)\n",
            "Dec 10 09:23:31.317 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 3840 samples for epoch #89 from 1 peers. ETA 8.86 sec (refresh in 2.21 sec)\n",
            "Dec 10 09:23:33.539 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 6112 samples for epoch #89 from 1 peers. ETA 3.90 sec (refresh in 0.98 sec)\n",
            "Dec 10 09:23:34.526 [\u001b[1m\u001b[34mINFO\u001b[0m] my_cifar_run accumulated 7104 samples for epoch #89 from 1 peers. ETA 2.86 sec (refresh in 0.72 sec)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import hivemind\n",
        "\n",
        "# Create dataset and model, same as in the basic tutorial\n",
        "# For this basic tutorial, we download only the training set\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "model = nn.Sequential(nn.Conv2d(3, 16, (5, 5)), nn.MaxPool2d(2, 2), nn.ReLU(),\n",
        "                      nn.Conv2d(16, 32, (5, 5)), nn.MaxPool2d(2, 2), nn.ReLU(),\n",
        "                      nn.Flatten(), nn.Linear(32 * 5 * 5, 10))\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Create DHT: a decentralized key-value storage shared between peers\n",
        "# dht = hivemind.DHT(start=True)\n",
        "\n",
        "# dht = hivemind.DHT(\n",
        "#     host_maddrs=[\"/ip4/0.0.0.0/tcp/0\", \"/ip4/0.0.0.0/udp/0/quic\"],\n",
        "#     initial_peers=[\n",
        "#         \"/ip4/3.140.223.7/tcp/16497/p2p/12D3KooWBCdJnh6r7n4L4zxCd3iEofpNDbJKLXQ5gq1DqK6RYbcH\",\n",
        "#     ], start=True)\n",
        "\n",
        "# dht = hivemind.DHT(\n",
        "#     host_maddrs=[\"/ip4/0.0.0.0/tcp/0\", \"/ip4/0.0.0.0/udp/0/quic\"],\n",
        "#     initial_peers=[\n",
        "#         f\"/ip4/{ip_address}/tcp/16497/p2p/12D3KooWBCdJnh6r7n4L4zxCd3iEofpNDbJKLXQ5gq1DqK6RYbcH\",\n",
        "#     ], start=True,client_mode=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"To join the training, use initial_peers =\", [str(addr) for addr in dht.get_visible_maddrs()])\n",
        "\n",
        "# Set up a decentralized optimizer that will average with peers in background\n",
        "opt = hivemind.Optimizer(\n",
        "    dht=dht,                  # use a DHT that is connected with other peers\n",
        "    run_id='my_cifar_run',    # unique identifier of this collaborative run\n",
        "    batch_size_per_step=32,   # each call to opt.step adds this many samples towards the next epoch\n",
        "    target_batch_size=10000,  # after peers collectively process this many samples, average weights and begin the next epoch\n",
        "    optimizer=opt,            # wrap the SGD optimizer defined above\n",
        "    use_local_updates=True,   # perform optimizer steps with local gradients, average parameters in background\n",
        "    matchmaking_time=3.0,     # when averaging parameters, gather peers in background for up to this many seconds\n",
        "    averaging_timeout=10.0,   # give up on averaging if not successful in this many seconds\n",
        "    verbose=True              # print logs incessently\n",
        ")\n",
        "\n",
        "# Note: if you intend to use GPU, switch to it only after the decentralized optimizer is created\n",
        "with tqdm() as progressbar:\n",
        "    while True:\n",
        "        for x_batch, y_batch in torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=32):\n",
        "            opt.zero_grad()\n",
        "            loss = F.cross_entropy(model(x_batch), y_batch)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            progressbar.desc = f\"loss = {loss.item():.3f}\"\n",
        "            progressbar.update()\n",
        "\n",
        "# num_epochs = 2  # 设置你想要的 epoch 数\n",
        "# for epoch in range(num_epochs):\n",
        "#     with tqdm(total=len(trainset), desc=f\"Epoch {epoch+1}/{num_epochs}\") as progressbar:\n",
        "#         for x_batch, y_batch in torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=32):\n",
        "#             opt.zero_grad()\n",
        "#             loss = F.cross_entropy(model(x_batch), y_batch)\n",
        "#             loss.backward()\n",
        "#             opt.step()\n",
        "\n",
        "#             progressbar.set_postfix(loss=loss.item())\n",
        "#             progressbar.update(len(x_batch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyJ-Vv002idw"
      },
      "source": [
        "To join the training, use initial_peers = ['/ip4/127.0.0.1/tcp/42905/p2p/12D3KooWSZVkzKhaTNAqohpaRbhXFGUbG18mbxczFktxLB4EEMwV']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJLVT7CD2i2r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02e3ae6e1e28406d90a8867e66424e64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e661dcbd6f44bfaa7f22c1354e6810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "080d26b376f243f6b33655b978f7d822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e682648ddb544baa373415eb141a9eb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1da0d0041ea84b989734b40d38cbda0c",
            "value": 1
          }
        },
        "0ea3abd6bb15432384c71d97f5819508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99e01f47b4744683a39650e920209823",
            "placeholder": "​",
            "style": "IPY_MODEL_adef8f315fe545ac9b8f7322cbaf76e4",
            "value": " 17216/? [18:38&lt;00:00, 32.83it/s]"
          }
        },
        "0f3a0a362a714d8bbb6f96eac7c9e2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eb16492fdbe468cb5e4833997176ef4",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05e661dcbd6f44bfaa7f22c1354e6810",
            "value": 170498071
          }
        },
        "18f0df7e5e844cf48109442f79851652": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc194b33400f4e65a64a071e59e9f430",
            "placeholder": "​",
            "style": "IPY_MODEL_756252a23c474f0981707478ea6d9ce3",
            "value": " 170499072/? [00:11&lt;00:00, 18588230.15it/s]"
          }
        },
        "1da0d0041ea84b989734b40d38cbda0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b147433804f429aafe1416e5e1056fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "756252a23c474f0981707478ea6d9ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eb16492fdbe468cb5e4833997176ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a0adb1c7234f458ba0ee2b0d96ea38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943cc0e49e1e4211b718440252381750": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a21052c85ece4b4fa65a6f0ce72c99ae",
            "placeholder": "​",
            "style": "IPY_MODEL_e65a65f7bcbc42828ae6601a7c2addd8",
            "value": ""
          }
        },
        "99e01f47b4744683a39650e920209823": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e682648ddb544baa373415eb141a9eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a21052c85ece4b4fa65a6f0ce72c99ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92722172922468b93e0065e8992a549": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c149fb64aee540518facfc37823db46b",
            "placeholder": "​",
            "style": "IPY_MODEL_4b147433804f429aafe1416e5e1056fd",
            "value": "loss = 0.583: "
          }
        },
        "abfa51b4884042b8ab465752d3e4a7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a92722172922468b93e0065e8992a549",
              "IPY_MODEL_080d26b376f243f6b33655b978f7d822",
              "IPY_MODEL_0ea3abd6bb15432384c71d97f5819508"
            ],
            "layout": "IPY_MODEL_02e3ae6e1e28406d90a8867e66424e64"
          }
        },
        "adef8f315fe545ac9b8f7322cbaf76e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c149fb64aee540518facfc37823db46b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4208eb89de342aba6b90408c476312f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_943cc0e49e1e4211b718440252381750",
              "IPY_MODEL_0f3a0a362a714d8bbb6f96eac7c9e2a2",
              "IPY_MODEL_18f0df7e5e844cf48109442f79851652"
            ],
            "layout": "IPY_MODEL_91a0adb1c7234f458ba0ee2b0d96ea38"
          }
        },
        "dc194b33400f4e65a64a071e59e9f430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65a65f7bcbc42828ae6601a7c2addd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}